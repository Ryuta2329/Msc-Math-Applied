---
title: Soluciones a problemas de _Time Series Analysis and its Applications_ de Shumway y Stoffer
author: Marcelo Molinatti
date: "`r Sys.Date()`"
output:
---

```{r setup}
library(astsa)
library(ggfortify)
```

# Capitulo 1.

**Problema 1.1** Para comparar las señales de terremotos y explosiones, represente los datos en el mismo gráfico usando diferentes colores o diferentes tipos de líneas y comente los resultados.

```{r p1-1, label="fig:p1-1", fig.caption=""}
p1 <- autoplot(cbind(EQ5, EXP6), facets=FALSE) +
	xlab("Índice de la muestra") + ylab("Amplitud") +
	scale_colour_manual(name="", values=c("20","200")) + 
	theme_light() +
	theme(legend.position = c(0.3, 0.85))
```

Lo primero que sale a la vista en el gráfico es que la variabilidad de las señales de terremoto es mayor (la amplitud de la señal es mayor) y que esta variabilidad se extiende por más tiempo en el registro, mientras que la de explosiones es menor la variabilidad y la amplitud solo se extiende un intervalo de tiempo corto. 

**Problema 1.2** Considere un modelo de señal más ruido de la forma general $x_t = s_t + w_t$, donde $w_t$ es ruido blanco gaussiano con $\sigma^2_w = 1$. Simule y grafique $n = 200$ observaciones de cada uno de los siguientes dos modelos.

_a)_ $x_t = s_t + w_t$, para $t = 1, \ldots , 200$, donde:

$$
s_t = \begin{cases}
    0, & t= 1, \ldots, 100\\
    10\text{exp}{-\frac{(t-100)}{20}}\text{cos}(2\pi t/4), & t=101, \ldots, 200
  \end{cases}
  \label{eq:p1-2-st_a}
$$

_b)_ $x_t = s_t + w_t$, para $t = 1, \ldots , 200$, donde

$$
s_t = \begin{cases}
    0, & t= 1, \ldots, 100\\
    10\text{exp}{-\frac{(t-100)}{200}}\text{cos}(2\pi t/4), & t=101, \ldots, 200
  \end{cases}
  \label{eq:p1-2-st_b}
$$

```{r p1-2, label="fig:p1-2", fig.caption=""}
xt_a <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 20) * cos(2 * pi * 101:200 / 4)) + rnorm(200, 0, 1)
xt_b <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 200) * cos(2 * pi * 101:200 / 4)) + rnorm(200, 0, 1)
xt <- ts(cbind(xt_a, xt_b), start=1)

p2 <- autoplot(xt, facets=TRUE) +
	xlab("Índice de la muestra") + ylab("Amplitud") + 
	theme_light()

cowplot::plot_grid(p1, p2, nrow=1)
```

_c)_ Compare la apariencia general de las series _a)_ y _b)_ con la serie de terremoto y la serie de explosión. Además, traza (o dibuja)
y compare los moduladores de señal _a)_ $exp{−t/20}$ y _b)_ $exp{−t/200}$, para $t = 1, 2, \ldots, 100$.

La serie en _a)_ parece describir de forma adecuada la fase P y S de las explosiones, dada la caida rapida de la amplitud al inicio de la fase S, y tambien notando que la variabilidad es bastane similar es esta serie con la serie de explosiones. Sun embargo, en la fase P, la serie del inciso _a)_ no parece realizar un buen trabajo en simular la serie de explosiones, principalemnte al inicio donde parece haber un cambio de variabilidad importante.

En el caso de la serie en el inciso _b)_, la fase S parece ser similar tambien en amplitud, aunque la serie parece ser mas regular y menos variable que la registrada en Escandinavia para la serie de terremoto. De igual forma, en la fase P la serie no tiene una variabilidad tan grande haciendo que la similitud en esta zona sea distinta (a lo mejor, un ruido aleatorio de varianza 1 no es lo suficientemente bueno para simular el proceso registrado en Escandinavia).

```{r p1-2-modulators, label="fig:p1-2-2", fig.caption=""}
modulators <- data.frame(mod_a=exp(-(1:100) / 20), mod_b=exp(-(1:100) / 200))

autoplot(ts(modulators, start=1), facets=FALSE) +
	xlab("t") + ylab(latex2exp::TeX("$exp\\{-t/\\tau\\}$")) + 
	scale_colour_manual(name=latex2exp::TeX("\\tau"), values=c("20","200"), labels=c("20","200")) + 
	theme_light()
```

Al ver los moduladores de las señales, se puede verificar que la caida exponencial es mas rapida en la señal del inciso _a)_, lo cual explica la rapida desaparicion de la señal en un intervalo corto del tiempo, mientras que la caida mas lenta del modulador de la señal del inciso _b)_ explica la persintencai de la señal en un intervalo de tiempo amplio.

**Problema 1.3** _a)_ Generar $n = 100$ observaciones a partir de la autorregresión

$$x_t = -.9x_{t-2} + w_t$$

con $\sigma_w = 1$. A continuación, aplique el filtro de promedio móvil $v_t = (x_t + x_{t−1} + x_{t−2} + x_{t−3})/4$ a $x_t$. Ahora trace $x_t$ como una línea y superponga $v_t$ como una línea discontinua. Comente sobre el comportamiento de $x_t$ y cómo la aplicación del filtro de promedio móvil cambia ese comportamiento.  
_b)_ Repite _a)_ pero con $x_t = cos(2πt/4)$.  
_c)_ Repite _b)_ pero con ruido $N(0, 1)$, $x_t = cos(2πt/4) + w_t$.  
_d)_ Compare y contraste _a)_–_c)_; i.e., como cambia el promedio móvil cada serie. 

```{r p1-3, label="fig:p1-3", fig.caption="", warning=FALSE, out.width="100%"}
sims <- data.frame(
  # Iniciso a)
  a=filter(rnorm(200, 0, 1), filter=c(0, -.9), method="recursive")[-(1:100)],
  # Iniciso b)
  b=cos(2 * pi * 1:100 / 4),
  # Iniciso c)
  c=cos(2 * pi * 1:100 / 4) + rnorm(200, 0, 1))

legend <- NULL
plot_list <- apply(sims, 2, function(ts) {
	# Aplico el filtrp sobre la serie:
	vt <- filter(ts, filter=rep(1/4, 4), sides=1, method="convolution")

	# Grafico
	p1 <- autoplot(cbind(ts, vt), facets=FALSE) +
	  xlab("Índice") + ylab("") + 
	  aes(linetype = plot_group) +
	  scale_linetype_manual(name="", labels=c(expression("x"["t"]), expression("v"["t"])), values=c(1, 2)) +
  	  scale_colour_manual(name="", values=c("20","200"), labels=c(expression("x"["t"]), expression("v"["t"]))) + 
	  theme_light()

	  legend <<- cowplot::get_legend(p1 + theme(legend.box.margin = margin(0, 0, 0, 1)))

	  p1
})

cowplot::plot_grid(
	cowplot::plot_grid(plotlist=lapply(plot_list, function(x) x + theme(legend.position="none")), 
		nrow=1, align = 'vh', labels = c("A", "B", "C"), hjust = -1), 
	legend, nrow = 1, rel_widths = c(3, .4))
```

Para el inciso _a)_, la serie antes del filtro muestra un cambio violento en el comportamiento de $x_t$, mientras que luego del filtro, la serie se suaviza y la variación de $v_t$ se ve menos pronunciada en amplitud (no se observan picos tan grandes).  
Para el inciso _b)_, la serie consiste de una función coseno regular, que luego de la suavización, se comporta como una constante, dado que el promediar elimina la variación dado que el periodo es de 4 unidades, y el promedio se hace con los 4 elementos inmediatamente en el pasado.  
Para el inciso _c)_, el ruido gaussiano agrega cierta variacion que quia la regularidad de la función coseno, por lo tanto la señal suavizada no se cancela en su totalidad, y esta muestra una variacion tambien, aunque su comportamiento es menos violeto. Sea como sea, aun se aprecia un póco el comportamiento de la onda coseno, tanto en $x_t$ como en $v_t$.  

Al comparar las tres series, podemos observar que el proceso de suavizar la serie cambia dependiendo de la forma de la señal subyacente. Cuando la señal varia entorno aun valor medio constante (como en el caso de la señal en A), el suavizado parece reducir la variabilidad en mayor medida que cuadno la media es variable (como en el caso de la señal en C).

**Problema 1.4** Demuestre que la función de autocovarianza se puede escribir como:

$$\gamma(s, t) = E[(x_s − \mu_s)(x_t − \mu_t)] = E(x_sx_t) − \mu_s\mu_t$$

Partiendo de la definición $E[(x_s − \mu_s)(x_t − \mu_t)]$, se distribuyen los terminos:

$$
\begin{aligned}
		\gamma(s, t) &= E[(x_s − \mu_s)(x_t − \mu_t)] \\
			&= E[x_sx_t − \mu_sx_t - x_s\mu_t + \mu_s\mu_t] \\
			&= E[x_sx_t] - \mu_sE[x_t] - \mu_tE[x_s] + \mu_s\mu_t \\
			&= E[x_sx_t] - \mu_s\mu_t - \mu_s\mu_t + \mu_s\mu_t \\
			&= E[x_sx_t] - \mu_s\mu_t
\end{aligned}
$$

**Problema 1.5** Para las dos series, $x_t$, en el Problema 1.2 _a)_ y _b)_:  
_a)_ Calcule y grafique las funciones medias $\mu_x(t)$, para $t = 1, . . . , 200$.
_b)_ Calcule las funciones de autocovarianza, $\gamma_x(s, t)$, para $s, t = 1, . . . , 200$.

Para ambos casos, la función media es:

$$
\begin{aligned}
E[x_t] &= E[s_t + w_t] \\
  &= E[s_t] + E[w_t] \\
  &= E[s_t] + 0 \\
  &= E[s_t]
\end{aligned}
$$

De forma que, para el inciso _a)_ $E[x_{t,a}]$ viene dada por \ref{eq:p1-2-st_a}, y para el inciso _b)_ $E[x_{t,b}]$ viene dada por \ref{eq:p1-2-st_b}. 

```{r p1-5, fig.caption=""}
xt_a <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 20) * cos(2 * pi * 101:200 / 4)) 
xt_b <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 200) * cos(2 * pi * 101:200 / 4)) 
xt <- ts(cbind(xt_a, xt_b), start=1)

autoplot(xt, facets=FALSE) +
	xlab("Índice de la muestra") + ylab(expression("E[x"["t"]*"]")) + 
	scale_colour_manual(name="", values=c("20","200"), labels=c("a", "b")) +
	theme_light()
```

La función de autocovarianza se puede encontrar usando el resultado del problema 1.4 y el resultado anterior:

$$
\begin{aligned}
	\gamma(s, t) &= E[x_tx_s] - \mu_t\mu_s \\
		  &= E[(s_t + w_t)(s_s + w_s)] - \mu_t^2 \\
		  &= E[s_ts_s + s_tw_s + s_sw_t + w_sw_t] - \mu_t^2 \\
		  &= E[s_ts_s] + s_tE[w_s] + s_sE[w_t] + E[w_sw_t] - \mu_t^2 \\
		  &= s_ts_s + 0 + 0 + E[w_s]E[w_t] - \mu_t^2 \\
		  &= s_ts_s - \mu_t^2
\end{aligned}
$$

**Problema 1.6** Considere la serie de tiempo: $x_t = \beta_1 + \beta_2t + w_t$, donde $\beta_1$ y $\beta_2$ son constantes conocidas y $w_t$ es un proceso de ruido blanco con varianza $\sigma_w^2$.

_a)_ Determine si $x_t$ es estacionario.
_b)_ Demuestre que el proceso $y_t = x_t − x_{t−1}$ es estacionario.
_c)_ Demuestre que la media del promedio móvil:

$$v_t = \frac{1}{2q + 1}\sum_{j=-q}^q x_{t-j}$$

es $\beta_1 + \beta_2t$, y dé una expresión simplificada para la función de autocovarianza.

Para el incisio _a)_, se puede demostrar que $x_t$ no es estacionario ya que $E[x_t] = E[\beta_1 + \beta_2t + w_t] = \beta_1 + \beta_2t + E[w_t] = \beta_1 + \beta_2t$ no es independiente del tiempo. Además, la función de autocovarianza es:

$$
\begin{aligned}
	\gamma(s, t) &= E[(x_{t} - \mu_t)(x_s - \mu_s)]\\
		&= E[(\beta_1 + \beta_2t + w_t - \beta_1 - \beta_2t)(\beta_1 + \beta_2s + w_s - \beta_1 - \beta_2s)] \\
		&= E[w_tw_s] 
\end{aligned}
$$

Cuando $s=t$, entonces $\gamma(s, t) = \sigma_w^2$. Si $s\ne t$, los ruidos aleatorios son independientes y por lo tanto $\gamma(s, t) = 0$. Esta no depdnede de las diferencias entre $s$ y $t$.  
Por otro lado, en el inciso _b)_ se tiene el proceso $y_t = x_t − x_{t−1}$, que se escribe en terminos de las variables $\beta$ como:

$$
\begin{aligned}
	y_t &= x_t − x_{t−1} \\
	  &= \beta_1 + \beta_2t + w_t - (\beta_1 + \beta_2(t-1) + w_{t-1}) \\
	  &= \beta_2 + (w_t - w_{t-1})
\end{aligned}
$$

cuya media es $E[y_t] = E[\beta_2 + (w_t - w_{t-1})] = \beta_2 + E[w_t - w_{t-1}] = \beta_2$. Su función de autocovarianza es:

$$
\begin{aligned}
	\gamma(s, t) &= E[(y_{t} - \mu_t)(y_s - \mu_s)]\\
		&= E[\beta_2 + (w_t - w_{t-1}) - \beta_2)(\beta_2 + (w_s - w_{s-1}) - \beta_2)] \\
		&= E[(w_t - w_{t-1})(w_s - w_{s-1})] 
\end{aligned}
$$

Cuando $s=t$, entonces $\gamma(s, t) = 2\sigma_w^2$. Cuando $s=t-1$, entonces $\gamma(s, t) = E[(w_t - w_{t-1})(w_{t-1} - w_{t-2})] = \sigma_w^2$; cuando $s \le t-2$, $\gamma(s, t) = 0$. Por lo que se puede escribir:

$$
gamma_y(s, t) = \begin{cases}
		2\sigma_w^2, & \vert h\vert = 0,
		\sigma_w^2, & \vert h \vert = 1,
		0, & \vert h \vert > 1 
	\end{cases}
$$

Por lo tanto el proceso $y_t$ es estacionario.

Para el promedio móvil del inciso _c)_, se tiene una media de:

$$
\begin{aligned}
	E[v_t] &= E[\frac{1}{2q + 1}\sum_{j=-q}^q x_{t-j}] \\
		&= \frac{1}{2q + 1}E\left[\sum_{j=-q}^q x_{t-j}\right] \\
		&= \frac{1}{2q + 1}E\left[\sum_{j=-q}^q \beta_1 + \beta_2(t-j) + w_{t-j}\right] \\
		&= \frac{1}{2q + 1}E\left[ (2q + 1)\beta_1 + (2q + 1)\beta_2t + \sum_{j=-q}^q w_{t-j}\right] \\
		&= \frac{1}{2q + 1}\left((2q + 1)\beta_1 + (2q + 1)\beta_2t + E\left[\sum_{j=-q}^q w_{t-j}\right] \right)\\
		&= \beta_1 + \beta_2t + \frac{1}{2q + 1}\sum_{j=-q}^q E[w_{t-j}] \\
		&= \beta_1 + \beta_2t
\end{aligned}
$$

Para la función de autocovarianza:

$$
\begin{aligned}
	\gamma_v(s, t) &= E[(v_{t} - \mu_t)(v_s - \mu_s)] \\
		&= E\left[\frac{1}{2q + 1}\sum_{j=-q}^q x_{t-j} - \beta_1-\beta_2t)(\frac{1}{2q + 1}\sum_{j=-q}^q x_{s-j} - \beta_1- \beta_2s)\right] \\
		&= \left(\frac{1}{2q + 1}\right)^2 E\left[\left( \sum_{j=-q}^q w_{t-j} \right)\left( \sum_{j=-q}^q w_{s-j} \right)\right]
\end{aligned}
$$

Cuando $s=t$, se tiene que $\gamma_v(s, t) = \frac{\sigma_w^2}{2q + 1}$. Si $s = t-1$, se tiene:

$$
\begin{aligned}
	E\left[\left( \sum_{j=-q}^q w_{t-j} \right)\left( \sum_{j=-q}^q w_{s-j} \right)\right] &= 
			E[(w_{t+q} + w_{t+q-1} + \ldots + w_{t-q+1} + w_{t-q})(w_{t+q-1} + w_{t+q-2} + \ldots + w_{t-q} + w_{t-q-1})] \\
		&= 2q\sigma_w^2
\end{aligned}
$$

Si $s=t-2$, se tiene que solo quedan $2q - 1$ terminos que no se anulan, y por lo tanto, $(2q - 1)\sigma_w^2$. De forma que se puede escribri la función de aitocovarianza en terminos de la diferencia $h=s-t$ como:

$$\gamma_v(s, t) = \left(\frac{2q + 1 - h}{(2q + 1)^2}\right)\sigma_w^2$$

lo cual muestra que la funcion de autocovarianza va decreciendo con la diferencia de tiempo $h$.

**Problema 1.7** Para un proceso de promedio móvil de la forma $x_t = w_{t−1} + 2w_t + w_{t+1}$, donde $w_t$ son independientes con medias cero y varianza $\sigma_w^2$, determine las funciones de autocovarianza y autocorrelación en función del desfase $h = s − t$ y grafique el ACF
en función de $h$.

La función de autocovarianza es (dado que la media es 0, ya que se trata de la suma de la media de tres ruidos blancos): $\gamma(t,s) = E[(w_{t−1} + 2w_t + w_{t+1})(w_{s−1} + 2w_s + w_{s+1})]$. Cuando $s=t$, se tiene:

$$
\begin{aligned}
	\gamma(t,t) &= E[(w_{t−1} + 2w_t + w_{t+1})(w_{t−1} + 2w_t + w_{t+1})] \\
		&= cov(w_{t-1}, w_{t-1}) + 2 cov(w_{t}, w_{t}) + cov(w_{t+1}, w_{t+1}) \\
		&= 4\sigma_w^2
\end{alogned}
$$

Cuando la diferencia entre $s$ y $t$ es 1, solo los terminos del centro y la derecha no se anulan, por lo que $\gamma(t,t\pm1) = 3\sigma_w^2$. Si la diferencia es de 2 solo un termino (el de la izquierda) no se cancela, por lo que $\gamma(t,t\pm2) = \sigma_w^2$. Para diferencias mayores que 2, la función de autocovarianza es cero. Queda entonces:

$$\gamma(t,s) = \begin{cases}
	4\sigma_w^2, & h=0 \\
	3\sigma_w^2, & h=\pm1 \\
	\sigma_w^2, & h=\pm2 \\
	0, & h=\pm3, \pm4, \ldots
\end{cases}$$

La función de autocorrelación queda:

$$\rho(t,s) = \begin{cases}
	1, & h=0 \\
	3/4, & h=\pm1 \\
	1/4, & h=\pm2 \\
	0, & h=\pm3, \pm4, \ldots
\end{cases}$$

El grafico de esta es:

```{r ex09, label="fig:p1-7"}
df <- data.frame(lag=-5:5, rho=c(0, 0, 0, .25, .75, 1, .75, .25, 0, 0, 0))

ggplot(data = df, mapping = aes(x = lag, y = rho)) +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_segment(mapping = aes(xend = lag, yend = 0)) +
  ylab(latex2exp::TeX("$\\rho(s, t)$")) +
  theme_light()
```

**Problema 1.8** Considere el modelo de paseo aleatorio con deriva $x_t = \delta + x_{t−1} + w_t$, para $t = 1, 2, \ldots$, con $x_0 = 0$, donde $w_t$ es ruido blanco con varianza $\sigma_w^2$.  
_a)_ Demuestre que el modelo se puede escribir como $x_t = \delta t + \sum_{k=1}^t w_k$.  
_b)_ Encuentre la función media y la función de autocovarianza de $x_t$.  
_c)_ Argumente que $x_t$ no es estacionario.  
_d)_ Muestre $\rho_x(t − 1, t) = \sqrt{\frac{t−1}{t}} \rightarrow 1$ cuando $t \rigtharrow \infty$. ¿Cuál es la implicación de este resultado?  
_e)_ Sugiera una transformación para hacer que la serie sea estacionaria y demuestre que la serie transformada es estacionaria.

Para el inciso _a)_, se puede rescribir el modelo usando la definicion del modelo de paseo alaetorio de forma recursiva, notando que en $t=0$ no hay una opbaservación, por lo que definimos $x_0=0$, quedando:

$$
\begin{aligned}
	x_t &= \delta + x_{t−1} + w_t \\
		&= \delta + (\delta + x_{t-2} + w_{t-1}) + w_t = 2\delta + x_{t-2} + w_t + w_{t-1} \\
		&= 2\delta + (\delta + x_{t-3} + w_{t-2}) + w_t + w_{t-1} = 3\delta + x_{t-3} + w_t + w_{t-1} + w_{t-2} \\
		&= \ldots \\
		&= (t-1)\delta + x_1 + \sum_{j=2}^t w_j = (t-1)\delta + (\delta + x_0 + w_1) + \sum_{j=2}^t w_j \\
		&= t\delta + \sum_{j=1}^t w_j
\end{aligned}
$$

lo cual demuestra la equivalencia.  
La función media del proceso es:

$$E[x_t] = E\left[t\delta + \sum_{j=1}^t w_j\right] = t\delta + E\left[\sum_{j=1}^t w_j\right] = t\delta + \sum_{j=1}^t E[w_j] = t\delta$$

y la función de autocovarianza:

$$
\begin{aligned}
	\gamma(t,s) &= E[(x_t - \mu_t)(x_s - \mu_s)] \\
		&= E\left[\left(t\delta + \sum_{j=1}^t w_j - t\delta\right)\left(s\delta + \sum_{j=1}^s w_j - s\delta\right)\right] \\
		&= E\left[\left(\sum_{j=1}^t w_j\right)\left(\sum_{j=1}^s w_j\right)\right] \\
		&= \text{min}\{s, t\} \sigma_w^2
\end{aligned}
$$

Como es posible ver de las funciones de media y autocovarianza, el proceso no es estacioanrio dado que la media depende del valor de $t$ y crece a medida que avamca el tiempo, sin cota alguna. Meintras que la función de autocvarianza no depende de la diferencia entre $s$ y $t$, sino del mínimo valor de estos. Por lo tanto, no es posible cumplir los requisitos de estacionaridad débil, y mucho menos al estacioanridad estricta.  
Se tiene que $\rho_x(t − 1, t) = \sqrt{\frac{t−1}{t}}$. Cuando $t \rightarrow \infty$, el numerador en la ACF tiende a $t$, haceindo que $\rho_x(t-1, t) \rightarrow 1$, lo cual implica una correlación perfecta entre el valor observado en $t$ y el inmediatamente adyacente, indicando en teoría, que sería posible el predecir con certeza el valor en $t$, conociendo el valor inmediatamente anterior.  
Una transformación posible es $y_t = x_t - x_{t-1}$, de tal forma que el modelo queda como $y_t=\delta + w_t$, cuya función media es $E[y_t] = E[\delta + w_t] = \delta$, una constante; y la función de autocovarianza es $\gamma_y(t,s) = E[(\delta + w_t - \delta)(\delta + w_s - \delta)] = E[(w_t)(w_s)]$ la cual es $\sigma_w^2$ cuando $h=t-s=0$, y 0 de otra forma. 

**Problema 1.9** Una serie de tiempo con un componente periódico se puede construir a partir de $x_t = U_1 sen(2\pi w_0t) + U_2 cos(2\pi w_0t)$, donde $U_1$ y $U_2$ son variables aleatorias independientes con media cero y $E(U_1^2) = E(U_2^2) = \sigma^2$. La constante $w_0$ determina el período o tiempo que tarda el proceso en realizar un ciclo completo. Demostrar que esta serie es débilmente estacionaria con función de autocovarianza:

$$\gamma(h) = \sigma^2cos(2\pi w_0 h)$$

Se tiene que la función media es:

$$
\begin{aligned}
	E[x_t] &= E[U_1 sen(2\pi w_0t) + U_2 cos(2\pi w_0t)] \\
		&= E[U_1 sen(2\pi w_0t)] + E[U_2 cos(2\pi w_0t)] \\
		&= E[U_1] sen(2\pi w_0t) + E[U_2] cos(2\pi w_0t) \\
		&= 0 + 0 = 0
\end{aligned}
$$

lo cual muestra que la media es constante e independiente de $t$. La función de autocovarianza es:

$$
\begin{aligned}
	\gamma(h) &= E[(x_{t+h} - E[x_t])(x_{t} - E[x_{t}])] \\
		&= E[(U_1 sen(2\pi w_0(t+h)) + U_2 cos(2\pi w_0(t+h)))(U_1 sen(2\pi w_0t) + U_2 cos(2\pi w_0t))] \\
		&= E\{U_1^2 sen(2\pi w_0(t+h))sen(2\pi w_0t) + U_1U_2 [sen(2\pi w_0(t+h))cos(2\pi w_0t) + sen(2\pi w_0t)cos(2\pi w_0(t+h))] + U_2^2 cos(2\pi w_0(t+h))cos(2\pi w_0t) \} \\
		&= E[U_1^2 sen(2\pi w_0(t+h))sen(2\pi w_0t)] + E[U_2^2 cos(2\pi w_0(t+h))cos(2\pi w_0t)] \\
		&= E[U_1^2]sen(2\pi w_0(t+h))sen(2\pi w_0t) + E[U_2^2]cos(2\pi w_0(t+h))cos(2\pi w_0t) \\
		&= \sigma^2 sen(2\pi w_0(t+h))sen(2\pi w_0t) + \sigma^2 cos(2\pi w_0(t+h))cos(2\pi w_0t) \\
		&= \sigma^2 [sen(2\pi w_0(t+h))sen(2\pi w_0t) + cos(2\pi w_0(t+h))cos(2\pi w_0t)] \\
		&= \sigma^2 cos(2\pi w_0 h)
\end{aligned}
$$

donde el termino central sea nula dado que $U_1$ y $U_2$ son independientes (tal que $E[U_1U_2] = E[U_1]E[U_2] = 0$) y la identidad trigonometrica para suma de angulos para el coseno ($cos(\alpha - \beta) = cos(\alpha)cos(\beta) + sen(\alpha)sen(\beta)$).

**Problema 1.10** Supongamos que nos gustaría predecir una sola serie estacionaria $x_t$ con media cero y función de autocorrelación $\rho(h)$ en algún momento en el futuro, digamos, $t + l$, para $l > 0$.  
_a)_ Si predecimos usando solo $x_t$ y algún multiplicador de escala $A$, demuestre que el error de predicción cuadrático medio $MSE(A) = E[(x_t+l − Ax_t)^2]$ es minimizado por $A = \rho(l)$.  
_b)_ Demuestre que el error de predicción cuadrático medio mínimo es $MSE(A) = \gamma(0)[1 − \rho^2(l)]$.  
_c)_ Demuestre que si $x_t+l = Ax_t$, entonces $\rho(l) = 1$ si $A > 0$, y $ρ(l) = −1$ si $A < 0$.



**Problema 1.11** Considere el proceso lineal definido en (1.31), como:

$$x_t = \mu + \sum_{j=-\infty}^\infty \psi_jw_{t-j}, \qquad \sum_{j=-\infty}^\infty \vert\psi_j\vert < \infty$$

_a)_ Verifique que la función de autocovarianza del proceso está dada por (1.32):  

$$\gamma_x(h) = \sigma_w^2 \sum_{j=-\infty}^\infty \psi_{j+h}\psi_j$$

Use el resultado para verificar su respuesta al Problema 1.7. Pista: Para $h \ge 0$, $cov(x_{t+h}, x_t) = cov(\sum_k \psi_k w_{t+h−k}, \sum_j \psi_jw_{t−j})$. Para cada $j \in Z$, el único _superviviente_ será cuando $k = h + j$.
_b)_ Demuestre que $x_t$ existe como un límite en el cuadrado medio (vea el Apéndice A).

**Problema 1.12** Para dos series débilmente estacionarias $x_t$ e $y_t$, verifique (1.30): $\rho_{xy}(h) = \rho_{yx}(−h)$.

**Problema 1.13** Considere las dos series $x_t = w_t$ y $y_t = w_t − \theta w_{t−1} + u_t$, donde $w_t$ y $u_t$ son series de ruido blanco independientes con varianzas $\sigma_w^2$ y $\sigma_u^2$, respectivamente, y $\theta$ es una constante no especificada.  
_a)_ Exprese el ACF, $\rho_y(h)$, para $h = 0, \pm1, \pm2, \ldots$ de la serie $y_t$ en función de $\sigma_w^2$, $\sigma_u^2$ y $\theta$.  
_b)_ Determine el CCF, $ρ_{xy}(h)$ relacionando $x_t$ y $y_t$.
_c)_ Demuestre que $x_t$ e $y_t$ son conjuntamente estacionarios.

**Problema 1.14** Sea $x_t$ un proceso normal estacionario con media $\mu_x$ y función de autocovarianza $\gamma(h)$. Definir la serie de tiempo no lineal $y_t = exp{x_t}$. _a)_ Exprese la función media $E(y_t)$ en términos de $\mu_x$ y $\gamma(0)$. La función generadora de momentos de una variable aleatoria normal $x$ con media $\mu$ y varianza $\sigma^2$ es:

$$M_x(λ) = E[exp{λx}] = exp\muλ + 1 2σ2λ2$$

_b)_ Determine la función de autocovarianza de $y_t$. La suma de las dos variables aleatorias normales $x_{t+h} + x_t$ sigue siendo una variable aleatoria normal.

**Problema 1.15** Sea $w_t$, para $t = 0, \pm1, \pm2, \ldots$ un proceso de ruido blanco normal, y considerar la serie $x_t = peso peso−1$.
Determine la media y la función de autocovarianza de $x_t$ e indique si es estacionaria.

**Problema 1.16** Considere la serie $x_t = sin(2\pi Ut)$, $t = 1, 2, \ldots$, donde $U$ tiene una distribución uniforme en el intervalo $(0, 1)$.  
_a)_ Demuestre que $x_t$ es débilmente estacionario.
_b)_ Demuestre que $x_t$ no es estrictamente estacionario.

**Problema 1.17** Supongamos que tenemos el proceso lineal xt generado por xt = wt − θwt−1, t = 0, 1, 2, . . ., donde {wt } es independiente e idénticamente distribuida con función característica φw(·), y θ es una constante fija. [Reemplazar “función característica” con “función generadora de momento” si se le indica que lo haga.]
(a) Exprese la función característica conjunta de x1, x2, . . . , xn, digamos, φx1,x2,...,xn(λ1, λ2, . . . , λn), en términos de φw(·).
(b) Deducir de (a) que xt es estrictamente estacionario.

**Problema 1.18** Suppose that xt is a linear process of the form (1.31). Prove
∞ 
h=−∞
|γ(h)| < ∞.

**Problema 1.19** Supongamos que xt = μ + wt + θwt−1, donde wt ∼ wn(0, σw2 ).
(a) Demuestre que la función media es E(xt) = μ.
(b) Demuestre que la función de autocovarianza de xt está dada por γx(0) = σw2 (1 + θ2),
γx(±1) = σw2 θ, y γx(h) = 0 en caso contrario.
(c) Demuestre que xt es estacionario para todos los valores de θ ∈ R.
(d) Use (1.35) para calcular var(x¯) para estimar μ cuando (i) θ = 1, (ii) θ = 0 y (iii) θ = −1
(e) En las series de tiempo, el tamaño de la muestra n suele ser grande, de modo que (n − n1) ≈ 1. Teniendo esto en cuenta, comente los resultados del inciso (d); en particular, ¿cómo cambia la precisión en la estimación de la media μ para los tres casos diferentes?

**Problema 1.20** (a) Simule una serie de n = 500 observaciones de ruido blanco gaussiano como en el ejemplo 1.8 y calcule el ACF de muestra, ˆ ρ(h), con un desfase de 20. Compare el ACF de muestra que obtenga con el ACF real, ρ(h). [Recuerde el Ejemplo 1.19.]
(b) Repita la parte (a) usando solo n = 50. ¿Cómo afecta el cambio de n a los resultados?

**Problema 1.21** (a) Simule una serie de n = 500 observaciones de promedio móvil como en el ejemplo 1.9 y calcule el ACF muestral, ˆ ρ(h), con un rezago de 20. Compare el ACF muestral que obtenga con el ACF real, ρ(h). [Recuerde el Ejemplo 1.20.]
(b) Repita la parte (a) usando solo n = 50. ¿Cómo afecta el cambio de n a los resultados?

**Problema 1.22** Aunque el modelo del problema 1.2(a) no es estacionario (¿por qué?), el ACF de muestra puede ser informativo. Para los datos que generó en ese problema, calcule y trace el ACF de muestra y luego comente.

**Problema 1.23** Simule una serie de n = 500 observaciones a partir del modelo de señal más ruido presentado en el Ejemplo 1.12 con σw2 = 1. Calcule el ACF de la muestra para retrasar 100 de los datos que generó y comente.

**Problema 1.24** Para la serie de tiempo yt descrita en el ejemplo 1.26, verifique el resultado establecido de que ρy(1) = −.47 y ρy(h) = 0 para h > 1.


**Problema 1.25** Una función de valor real g(t), definida en los números enteros, es definida no negativa si y solo si

aig(ti − tj)aj ≥ 0 

para todos los enteros positivos n y para todos los vectores a = (a1, a2, . . . , an)′ y t = (t1, t2, . . . , tn)′. Para la matriz G = {g(ti − tj); yo, j = 1, 2, . . . , n}, esto implica que a′Ga ≥ 0 para todos los vectores a. Se llama definido positivo si podemos reemplazar '≥' con '>' para todo un 0, el vector cero.
(a) Demuestre que γ(h), la función de autocovarianza de un proceso estacionario, es una función definida no negativa.
(b) Verifique que la autocovarianza muestral ˆ γ(h) es una función definida no negativa.

**Problema 1.26** Considere una colección de series de tiempo x1t, x2t, . . . , xNt que están observando alguna señal común μt observada en procesos de ruido e1t, e2t, . . . , eNt, con un modelo para la j-ésima serie observada dado por
xjt = μt + ejt.
Suponga que las series de ruido tienen medias cero y no están correlacionadas para diferentes j. Las funciones de autocovarianza comunes de todas las series vienen dadas por γe(s, t). Definir la media muestral
x¯t =
1 norte
Nj=1
xjt.
(a) Demuestre que E[x¯t] = μt.
(b) Demuestre que E[(x¯t − μ)2)] = N−1γe(t, t).
(c) ¿Cómo podemos usar los resultados para estimar la señal común?

**Problema 1.27** Un concepto utilizado en geoestadística, véase Journel y Huijbregts [109] o Cressie [45], es el de variograma, definido para un proceso espacial xs, s = (s1, s2), para s1, s2 = 0, ±1, ±2, . . ., como
Vx
(h) = 1
2
E[(xs+h − xs)2],
donde h = (h1, h2), para h1, h2 = 0, ±1, ±2, . . . Muestre que, para un proceso estacionario, las funciones de variograma y autocovarianza pueden relacionarse mediante Vx(h) = γ(0) − γ(h),
donde γ(h) es la función habitual de covarianza del retraso h y 0 = (0, 0). Tenga en cuenta la fácil extensión a cualquier dimensión espacial.

**Problema 1.28** Supongamos que xt = β0 + β1t, donde β0 y β1 son constantes. Demostrar como n → ∞, ρˆx(h) → 1 para h fija, donde ˆ ρx(h) es el ACF (1.37).

**Problema 1.29** a) Suponga que xt es una serie de tiempo débilmente estacionaria con media cero y con
función de autocovarianza absolutamente sumable, γ(h), tal que
∞
h=−∞
γ(h) = 0.
Demuestre que √n x¯ →p 0, donde ¯ x es la media muestral (1.34).
(b) Dé un ejemplo de un proceso que satisfaga las condiciones de la parte (a). ¿Qué tiene de especial este proceso?

**Problema 1.30** 

**Problema 1.31** 

**Problema 1.32** 

# Capitulo 2.

**Problema 2.1** Un modelo estructural Para los datos de Johnson & Johnson, digamos yt, que se muestran en la figura 1.1, sea xt = log(yt). En este problema, vamos a ajustar un tipo especial de modelo estructural, xt = Tt + St + Nt donde Tt es un componente de tendencia, St es un componente estacional y Nt es ruido. En nuestro caso, el tiempo t está en trimestres (1960.00, 1960.25, . . . ) por lo que una unidad de tiempo es un año.
(a) Ajuste el modelo de regresión

donde Qi(t) = 1 si el tiempo t corresponde al trimestre i = 1, 2, 3, 4 y cero en caso contrario. Las Qi(t) se denominan variables indicadoras. Supondremos por ahora que wt es una secuencia de ruido blanco gaussiana. Sugerencia: el código detallado se proporciona en el Código R.4, el último ejemplo de la Secc. R.4.
(b) Si el modelo es correcto, ¿cuál es el incremento anual promedio estimado en las ganancias registradas por acción?
(c) Si el modelo es correcto, ¿la tasa promedio de ganancias registradas aumenta o disminuye del tercer trimestre al cuarto trimestre? y ¿en qué porcentaje aumenta o disminuye?
(d) ¿Qué sucede si incluye un término de intersección en el modelo en (a)? Explique por qué hubo un problema.
(e) Grafique los datos, xt, y superponga los valores ajustados, digamos ˆ xt, en el gráfico. Examine los residuos, xt − xˆt, y establezca sus conclusiones. ¿Parece que el modelo se ajusta bien a los datos (los residuos se ven blancos)?