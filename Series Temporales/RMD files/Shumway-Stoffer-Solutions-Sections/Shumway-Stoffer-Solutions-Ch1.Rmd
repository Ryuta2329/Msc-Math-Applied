```{r ch1-setup}
suppressPackageStartupMessages({
	library(astsa)
	library(ggfortify)
})
```

**Problema 1.1** Para comparar las señales de terremotos y explosiones, represente los datos en el mismo gráfico usando diferentes colores o diferentes tipos de líneas y comente los resultados.

```{r p1-1, fig.caption=""}
p1 <- autoplot(cbind(EQ5, EXP6), facets = FALSE) +
  xlab("Índice de la muestra") +
  ylab("Amplitud") +
  scale_colour_manual(name = "",
    values = c("20", "200")) +
  theme_light() +
  theme(legend.position = c(0.3, 0.85))

p1
```

Lo primero que sale a la vista en el gráfico es que la variabilidad de las señales de terremoto es mayor (la amplitud de la señal es mayor) y que esta variabilidad se extiende por más tiempo en el registro, mientras que la de explosiones es menor la variabilidad y la amplitud solo se extiende un intervalo de tiempo corto. 

**Problema 1.2** Considere un modelo de señal más ruido de la forma general $x_t = s_t + w_t$, donde $w_t$ es ruido blanco gaussiano con $\sigma^2_w = 1$. Simule y grafique $n = 200$ observaciones de cada uno de los siguientes dos modelos.

_a)_ $x_t = s_t + w_t$, para $t = 1, \ldots , 200$, donde:

$$s_t = \begin{cases}
    0, & t= 1, \ldots, 100\\
    10\text{exp}{-\frac{(t-100)}{20}}\text{cos}(2\pi t/4), & t=101, \ldots, 200
  \end{cases}$$

_b)_ $x_t = s_t + w_t$, para $t = 1, \ldots , 200$, donde

$$s_t = \begin{cases}
    0, & t= 1, \ldots, 100\\
    10\text{exp}{-\frac{(t-100)}{200}}\text{cos}(2\pi t/4), & t=101, \ldots, 200
  \end{cases}$$

```{r p1-2, label="fig:p1-2", fig.caption=""}
xt_a <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 20) * cos(2 * pi * 101:200 / 4)) + rnorm(200, 0, 1)
xt_b <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 200) * cos(2 * pi * 101:200 / 4)) + rnorm(200, 0, 1)
xt <- ts(cbind(xt_a, xt_b), start = 1)

p2 <- autoplot(xt, facets = TRUE) +
  xlab("Índice de la muestra") + ylab("Amplitud") + 
  theme_light()

cowplot::plot_grid(p1, p2, nrow=1)
```

_c)_ Compare la apariencia general de las series _a)_ y _b)_ con la serie de terremoto y la serie de explosión. Además, traza (o dibuja)
y compare los moduladores de señal _a)_ $exp\{−t/20\}$ y _b)_ $exp\{−t/200\}$, para $t = 1, 2, \ldots, 100$.

La serie en _a)_ parece describir de forma adecuada la fase P y S de las explosiones, dada la caida rapida de la amplitud al inicio de la fase S, y tambien notando que la variabilidad es bastane similar es esta serie con la serie de explosiones. Sun embargo, en la fase P, la serie del inciso _a)_ no parece realizar un buen trabajo en simular la serie de explosiones, principalemnte al inicio donde parece haber un cambio de variabilidad importante.

En el caso de la serie en el inciso _b)_, la fase S parece ser similar tambien en amplitud, aunque la serie parece ser mas regular y menos variable que la registrada en Escandinavia para la serie de terremoto. De igual forma, en la fase P la serie no tiene una variabilidad tan grande haciendo que la similitud en esta zona sea distinta (a lo mejor, un ruido aleatorio de varianza 1 no es lo suficientemente bueno para simular el proceso registrado en Escandinavia).

```{r p1-2-modulators, label="fig:p1-2-2", fig.caption=""}
modulators <- data.frame(mod_a=exp(-(1:100) / 20), mod_b=exp(-(1:100) / 200))

autoplot(ts(modulators, start=1), facets=FALSE) +
  xlab("t") + ylab(latex2exp::TeX("$exp\\{-t/\\tau\\}$")) + 
  scale_colour_manual(name = latex2exp::TeX("\\tau"),
  	values = c("20", "200"), labels = c("20", "200")) + 
  theme_light()
```

Al ver los moduladores de las señales, se puede verificar que la caida exponencial es mas rapida en la señal del inciso _a)_, lo cual explica la rapida desaparicion de la señal en un intervalo corto del tiempo, mientras que la caida mas lenta del modulador de la señal del inciso _b)_ explica la persintencai de la señal en un intervalo de tiempo amplio.

**Problema 1.3** _a)_ Generar $n = 100$ observaciones a partir de la autorregresión

$$x_t = -.9x_{t-2} + w_t$$

con $\sigma_w = 1$. A continuación, aplique el filtro de promedio móvil $v_t = (x_t + x_{t−1} + x_{t−2} + x_{t−3})/4$ a $x_t$. Ahora trace $x_t$ como una línea y superponga $v_t$ como una línea discontinua. Comente sobre el comportamiento de $x_t$ y cómo la aplicación del filtro de promedio móvil cambia ese comportamiento.  
_b)_ Repite _a)_ pero con $x_t = cos(2πt/4)$.  
_c)_ Repite _b)_ pero con ruido $N(0, 1)$, $x_t = cos(2πt/4) + w_t$.  
_d)_ Compare y contraste _a)_–_c)_; i.e., como cambia el promedio móvil cada serie. 

```{r p1-3, label="fig:p1-3", fig.caption="", warning=FALSE, out.width="100%"}
sims <- data.frame(
  # Iniciso a)
  a=stats::filter(rnorm(200, 0, 1), filter=c(0, -.9), method="recursive")[-(1:100)],
  # Iniciso b)
  b=cos(2 * pi * 1:100 / 4),
  # Iniciso c)
  c=cos(2 * pi * 1:100 / 4) + rnorm(200, 0, 1))

legend <- NULL
plot_list <- apply(sims, 2, function(ts) {
	# Aplico el filtrp sobre la serie:
	vt <- stats::filter(ts, filter=rep(1/4, 4), sides=1, method="convolution")

	# Grafico
	p1 <- autoplot(cbind(ts, vt), facets=FALSE) +
	  xlab("Índice") + ylab("") + 
	  aes(linetype = plot_group) +
	  scale_linetype_manual(name="", labels=c(expression("x"["t"]), expression("v"["t"])), values=c(1, 2)) +
  	scale_colour_manual(name="", values=c("20","1"), labels=c(expression("x"["t"]), expression("v"["t"]))) + 
	  theme_light()

	  legend <<- cowplot::get_legend(p1 + theme(legend.box.margin = margin(0, 0, 0, 1)))

	  p1
})

cowplot::plot_grid(
	cowplot::plot_grid(plotlist=lapply(plot_list, function(x) x + theme(legend.position="none")), 
		nrow=1, align = 'vh', labels = c("A", "B", "C"), hjust = -1), 
	legend, nrow = 1, rel_widths = c(3, .4))
```

Para el inciso _a)_, la serie antes del filtro muestra un cambio violento en el comportamiento de $x_t$, mientras que luego del filtro, la serie se suaviza y la variación de $v_t$ se ve menos pronunciada en amplitud (no se observan picos tan grandes).  
Para el inciso _b)_, la serie consiste de una función coseno regular, que luego de la suavización, se comporta como una constante, dado que el promediar elimina la variación dado que el periodo es de 4 unidades, y el promedio se hace con los 4 elementos inmediatamente en el pasado.  
Para el inciso _c)_, el ruido gaussiano agrega cierta variacion que quia la regularidad de la función coseno, por lo tanto la señal suavizada no se cancela en su totalidad, y esta muestra una variacion tambien, aunque su comportamiento es menos violeto. Sea como sea, aun se aprecia un póco el comportamiento de la onda coseno, tanto en $x_t$ como en $v_t$.  

Al comparar las tres series, podemos observar que el proceso de suavizar la serie cambia dependiendo de la forma de la señal subyacente. Cuando la señal varia entorno aun valor medio constante (como en el caso de la señal en A), el suavizado parece reducir la variabilidad en mayor medida que cuadno la media es variable (como en el caso de la señal en C).

**Problema 1.4** Demuestre que la función de autocovarianza se puede escribir como:

$$\gamma(s, t) = E[(x_s − \mu_s)(x_t − \mu_t)] = E(x_sx_t) − \mu_s\mu_t$$

Partiendo de la definición $E[(x_s − \mu_s)(x_t − \mu_t)]$, se distribuyen los terminos:

$$
\begin{aligned}
		\gamma(s, t) &= E[(x_s − \mu_s)(x_t − \mu_t)] \\
			&= E[x_sx_t − \mu_sx_t - x_s\mu_t + \mu_s\mu_t] \\
			&= E[x_sx_t] - \mu_sE[x_t] - \mu_tE[x_s] + \mu_s\mu_t \\
			&= E[x_sx_t] - \mu_s\mu_t - \mu_s\mu_t + \mu_s\mu_t \\
			&= E[x_sx_t] - \mu_s\mu_t
\end{aligned}
$$

**Problema 1.5** Para las dos series, $x_t$, en el Problema 1.2 _a)_ y _b)_:  
_a)_ Calcule y grafique las funciones medias $\mu_x(t)$, para $t = 1, . . . , 200$.
_b)_ Calcule las funciones de autocovarianza, $\gamma_x(s, t)$, para $s, t = 1, . . . , 200$.

Para ambos casos, la función media es:

$$
\begin{aligned}
E[x_t] &= E[s_t + w_t] \\
  &= E[s_t] + E[w_t] \\
  &= E[s_t] + 0 \\
  &= E[s_t]
\end{aligned}
$$

```{r p1-5, fig.caption=""}
xt_a <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 20) * cos(2 * pi * 101:200 / 4)) 
xt_b <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 200) * cos(2 * pi * 101:200 / 4)) 
xt <- ts(cbind(xt_a, xt_b), start=1)

autoplot(xt, facets=FALSE) +
	xlab("Índice de la muestra") + ylab(expression("E[x"["t"]*"]")) + 
	scale_colour_manual(name="", values=c("20","200"), labels=c("a", "b")) +
	theme_light()
```

La función de autocovarianza se puede encontrar usando el resultado del problema 1.4 y el resultado anterior:

$$
\begin{aligned}
	\gamma(s, t) &= E[x_tx_s] - \mu_t\mu_s \\
		  &= E[(s_t + w_t)(s_s + w_s)] - \mu_t^2 \\
		  &= E[s_ts_s + s_tw_s + s_sw_t + w_sw_t] - \mu_t^2 \\
		  &= E[s_ts_s] + s_tE[w_s] + s_sE[w_t] + E[w_sw_t] - \mu_t^2 \\
		  &= s_ts_s + 0 + 0 + E[w_s]E[w_t] - \mu_t^2 \\
		  &= s_ts_s - \mu_t^2
\end{aligned}
$$

**Problema 1.6** Considere la serie de tiempo: $x_t = \beta_1 + \beta_2t + w_t$, donde $\beta_1$ y $\beta_2$ son constantes conocidas y $w_t$ es un proceso de ruido blanco con varianza $\sigma_w^2$.

_a)_ Determine si $x_t$ es estacionario.
_b)_ Demuestre que el proceso $y_t = x_t − x_{t−1}$ es estacionario.
_c)_ Demuestre que la media del promedio móvil:

$$v_t = \frac{1}{2q + 1}\sum_{j=-q}^q x_{t-j}$$

es $\beta_1 + \beta_2t$, y dé una expresión simplificada para la función de autocovarianza.

Para el inciso _a)_, se puede demostrar que $x_t$ no es estacionario ya que $E[x_t] = E[\beta_1 + \beta_2t + w_t] = \beta_1 + \beta_2t + E[w_t] = \beta_1 + \beta_2t$ no es independiente del tiempo. Además, la función de autocovarianza es:

$$
\begin{aligned}
	\gamma(s, t) &= E[(x_{t} - \mu_t)(x_s - \mu_s)]\\
		&= E[(\beta_1 + \beta_2t + w_t - \beta_1 - \beta_2t)(\beta_1 + \beta_2s + w_s - \beta_1 - \beta_2s)] \\
		&= E[w_tw_s] 
\end{aligned}
$$

Cuando $s=t$, entonces $\gamma(s, t) = \sigma_w^2$. Si $s\ne t$, los ruidos aleatorios son independientes y por lo tanto $\gamma(s, t) = 0$. Esta no depende de las diferencias entre $s$ y $t$.  
Por otro lado, en el inciso _b)_ se tiene el proceso $y_t = x_t − x_{t−1}$, que se escribe en términos de las variables $\beta$ como:

$$
\begin{aligned}
	y_t &= x_t − x_{t−1} \\
	  &= \beta_1 + \beta_2t + w_t - (\beta_1 + \beta_2(t-1) + w_{t-1}) \\
	  &= \beta_2 + (w_t - w_{t-1})
\end{aligned}
$$

cuya media es $E[y_t] = E[\beta_2 + (w_t - w_{t-1})] = \beta_2 + E[w_t - w_{t-1}] = \beta_2$. Su función de autocovarianza es:

$$
\begin{aligned}
	\gamma(s, t) &= E[(y_{t} - \mu_t)(y_s - \mu_s)]\\
		&= E[\beta_2 + (w_t - w_{t-1}) - \beta_2)(\beta_2 + (w_s - w_{s-1}) - \beta_2)] \\
		&= E[(w_t - w_{t-1})(w_s - w_{s-1})] 
\end{aligned}
$$

Cuando $s=t$, entonces $\gamma(s, t) = 2\sigma_w^2$. Cuando $s=t-1$, entonces $\gamma(s, t) = E[(w_t - w_{t-1})(w_{t-1} - w_{t-2})] = \sigma_w^2$; cuando $s \le t-2$, $\gamma(s, t) = 0$. Por lo que se puede escribir:

$$
\gamma_y(s, t) = \begin{cases}
		2\sigma_w^2, & \vert h\vert = 0,
		\sigma_w^2, & \vert h \vert = 1,
		0, & \vert h \vert > 1 
	\end{cases}
$$

Por lo tanto el proceso $y_t$ es estacionario.

Para el promedio móvil del inciso _c)_, se tiene una media de:

$$
\begin{aligned}
	E[v_t] &= E[\frac{1}{2q + 1}\sum_{j=-q}^q x_{t-j}] \\
		&= \frac{1}{2q + 1}E\left[\sum_{j=-q}^q x_{t-j}\right] \\
		&= \frac{1}{2q + 1}E\left[\sum_{j=-q}^q \beta_1 + \beta_2(t-j) + w_{t-j}\right] \\
		&= \frac{1}{2q + 1}E\left[ (2q + 1)\beta_1 + (2q + 1)\beta_2t + \sum_{j=-q}^q w_{t-j}\right] \\
		&= \frac{1}{2q + 1}\left((2q + 1)\beta_1 + (2q + 1)\beta_2t + E\left[\sum_{j=-q}^q w_{t-j}\right] \right)\\
		&= \beta_1 + \beta_2t + \frac{1}{2q + 1}\sum_{j=-q}^q E[w_{t-j}] \\
		&= \beta_1 + \beta_2t
\end{aligned}
$$

Para la función de autocovarianza:

$$
\begin{aligned}
	\gamma_v(s, t) &= E[(v_{t} - \mu_t)(v_s - \mu_s)] \\
		&= E\left[\frac{1}{2q + 1}\sum_{j=-q}^q x_{t-j} - \beta_1-\beta_2t)(\frac{1}{2q + 1}\sum_{j=-q}^q x_{s-j} - \beta_1- \beta_2s)\right] \\
		&= \left(\frac{1}{2q + 1}\right)^2 E\left[\left( \sum_{j=-q}^q w_{t-j} \right)\left( \sum_{j=-q}^q w_{s-j} \right)\right]
\end{aligned}
$$

Cuando $s=t$, se tiene que $\gamma_v(s, t) = \frac{\sigma_w^2}{2q + 1}$. Si $s = t-1$, se tiene:

$$
\begin{aligned}
	E\left[\left( \sum_{j=-q}^q w_{t-j} \right)\left( \sum_{j=-q}^q w_{s-j} \right)\right] &= 
			E[(w_{t+q} + w_{t+q-1} + \ldots + w_{t-q+1} + w_{t-q})(w_{t+q-1} + w_{t+q-2} + \ldots + w_{t-q} + w_{t-q-1})] \\
		&= 2q\sigma_w^2
\end{aligned}
$$

Si $s=t-2$, se tiene que solo quedan $2q - 1$ términos que no se anulan, y por lo tanto, $(2q - 1)\sigma_w^2$. De forma que se puede escribir la función de autocovarianza en términos de la diferencia $h=s-t$ como:

$$\gamma_v(s, t) = \left(\frac{2q + 1 - h}{(2q + 1)^2}\right)\sigma_w^2$$

lo cual muestra que la función de autocovarianza va decreciendo con la diferencia de tiempo $h$.

**Problema 1.7** Para un proceso de promedio móvil de la forma $x_t = w_{t−1} + 2w_t + w_{t+1}$, donde $w_t$ son independientes con medias cero y varianza $\sigma_w^2$, determine las funciones de autocovarianza y autocorrelación en función del desfase $h = s − t$ y grafique el ACF en función de $h$.

La función de autocovarianza es (dado que la media es 0, ya que se trata de la suma de la media de tres ruidos blancos): $\gamma(t,s) = E[(w_{t−1} + 2w_t + w_{t+1})(w_{s−1} + 2w_s + w_{s+1})]$. Cuando $s=t$, se tiene:

$$
\begin{aligned}
	\gamma(t,t) &= E[(w_{t−1} + 2w_t + w_{t+1})(w_{t−1} + 2w_t + w_{t+1})] \\
		&= cov(w_{t-1}, w_{t-1}) + 2 cov(w_{t}, w_{t}) + cov(w_{t+1}, w_{t+1}) \\
		&= 4\sigma_w^2
\end{aligned}
$$

Cuando la diferencia entre $s$ y $t$ es 1, solo los términos del centro y la derecha no se anulan, por lo que $\gamma(t,t\pm1) = 3\sigma_w^2$. Si la diferencia es de 2 solo un termino (el de la izquierda) no se cancela, por lo que $\gamma(t,t\pm2) = \sigma_w^2$. Para diferencias mayores que 2, la función de autocovarianza es cero. Queda entonces:

$$\gamma(t,s) = \begin{cases}
	4\sigma_w^2, & h=0 \\
	3\sigma_w^2, & h=\pm1 \\
	\sigma_w^2, & h=\pm2 \\
	0, & h=\pm3, \pm4, \ldots
\end{cases}$$

La función de autocorrelación queda:

$$\rho(t,s) = \begin{cases}
	1, & h=0 \\
	3/4, & h=\pm1 \\
	1/4, & h=\pm2 \\
	0, & h=\pm3, \pm4, \ldots
\end{cases}$$

El gráfico de esta es:

```{r ex09, label="fig:p1-7"}
df <- data.frame(lag=-5:5, rho=c(0, 0, 0, .25, .75, 1, .75, .25, 0, 0, 0))

ggplot(data = df, mapping = aes(x = lag, y = rho)) +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_segment(mapping = aes(xend = lag, yend = 0)) +
  ylab(latex2exp::TeX("$\\rho(s, t)$")) +
  theme_light()
```

**Problema 1.8** Considere el modelo de paseo aleatorio con deriva $x_t = \delta + x_{t−1} + w_t$, para $t = 1, 2, \ldots$, con $x_0 = 0$, donde $w_t$ es ruido blanco con varianza $\sigma_w^2$.  
_a)_ Demuestre que el modelo se puede escribir como:

$$x_t = \delta t + \sum_{k=1}^t w_k$$

_b)_ Encuentre la función media y la función de autocovarianza de $x_t$.  
_c)_ Argumente que $x_t$ no es estacionario.  
_d)_ Muestre $\rho_x(t − 1, t) = \sqrt{\frac{t−1}{t}} \rightarrow 1$ cuando $t \rightarrow \infty$. ¿Cuál es la implicación de este resultado?  
_e)_ Sugiera una transformación para hacer que la serie sea estacionaria y demuestre que la serie transformada es estacionaria.

Para el inciso _a)_, se puede rescribir el modelo usando la definición del modelo de paseo aleatorio de forma recursiva, notando que en $t=0$ no hay una observación, por lo que definimos $x_0=0$, quedando:

$$
\begin{aligned}
	x_t &= \delta + x_{t−1} + w_t \\
		&= \delta + (\delta + x_{t-2} + w_{t-1}) + w_t = 2\delta + x_{t-2} + w_t + w_{t-1} \\
		&= 2\delta + (\delta + x_{t-3} + w_{t-2}) + w_t + w_{t-1} = 3\delta + x_{t-3} + w_t + w_{t-1} + w_{t-2} \\
		&= \ldots \\
		&= (t-1)\delta + x_1 + \sum_{j=2}^t w_j = (t-1)\delta + (\delta + x_0 + w_1) + \sum_{j=2}^t w_j \\
		&= t\delta + \sum_{j=1}^t w_j
\end{aligned}
$$

lo cual demuestra la equivalencia.  
La función media del proceso es:

$$E[x_t] = E\left[t\delta + \sum_{j=1}^t w_j\right] = t\delta + E\left[\sum_{j=1}^t w_j\right] = t\delta + \sum_{j=1}^t E[w_j] = t\delta$$

y la función de autocovarianza:

$$
\begin{aligned}
	\gamma(t,s) &= E[(x_t - \mu_t)(x_s - \mu_s)] \\
		&= E\left[\left(t\delta + \sum_{j=1}^t w_j - t\delta\right)\left(s\delta + \sum_{j=1}^s w_j - s\delta\right)\right] \\
		&= E\left[\left(\sum_{j=1}^t w_j\right)\left(\sum_{j=1}^s w_j\right)\right] \\
		&= \text{min}\left{s,t\right} \sigma_w^2
\end{aligned}
$$

Como es posible ver de las funciones de media y autocovarianza, el proceso no es estacionario dado que la media depende del valor de $t$ y crece a medida que avanza el tiempo, sin cota alguna. Mientras que la función de autocovarianza no depende de la diferencia entre $s$ y $t$, sino del mínimo valor de estos. Por lo tanto, no es posible cumplir los requisitos de estacionaridad débil, y mucho menos al estacionaridad estricta.  
Se tiene que $\rho_x(t − 1, t) = \sqrt{\frac{t−1}{t}}$. Cuando $t \rightarrow \infty$, el numerador en la ACF tiende a $t$, haciendo que $\rho_x(t-1, t) \rightarrow 1$, lo cual implica una correlación perfecta entre el valor observado en $t$ y el inmediatamente adyacente, indicando en teoría, que sería posible el predecir con certeza el valor en $t$, conociendo el valor inmediatamente anterior.  
Una transformación posible es $y_t = x_t - x_{t-1}$, de tal forma que el modelo queda como $y_t=\delta + w_t$, cuya función media es $E[y_t] = E[\delta + w_t] = \delta$, una constante; y la función de autocovarianza es $\gamma_y(t,s) = E[(\delta + w_t - \delta)(\delta + w_s - \delta)] = E[(w_t)(w_s)]$ la cual es $\sigma_w^2$ cuando $h=t-s=0$, y 0 de otra forma. 

**Problema 1.9** Una serie de tiempo con un componente periódico se puede construir a partir de $x_t = U_1 sen(2\pi w_0t) + U_2 cos(2\pi w_0t)$, donde $U_1$ y $U_2$ son variables aleatorias independientes con media cero y $E(U_1^2) = E(U_2^2) = \sigma^2$. La constante $w_0$ determina el período o tiempo que tarda el proceso en realizar un ciclo completo. Demostrar que esta serie es débilmente estacionaria con función de autocovarianza:

$$\gamma(h) = \sigma^2cos(2\pi w_0 h)$$

Se tiene que la función media es:

$$
\begin{aligned}
	E[x_t] &= E[U_1 sen(2\pi w_0t) + U_2 cos(2\pi w_0t)] \\
		&= E[U_1 sen(2\pi w_0t)] + E[U_2 cos(2\pi w_0t)] \\
		&= E[U_1] sen(2\pi w_0t) + E[U_2] cos(2\pi w_0t) \\
		&= 0 + 0 = 0
\end{aligned}
$$

lo cual muestra que la media es constante e independiente de $t$. La función de autocovarianza es:

$$
\begin{aligned}
	\gamma(h) &= E[(x_{t+h} - E[x_t])(x_{t} - E[x_{t}])] \\
		&= E[(U_1 sen(2\pi w_0(t+h)) + U_2 cos(2\pi w_0(t+h)))(U_1 sen(2\pi w_0t) + U_2 cos(2\pi w_0t))] \\
		&= E\{U_1^2 sen(2\pi w_0(t+h))sen(2\pi w_0t) + U_1U_2 [sen(2\pi w_0(t+h))cos(2\pi w_0t) + sen(2\pi w_0t)cos(2\pi w_0(t+h))] + U_2^2 cos(2\pi w_0(t+h))cos(2\pi w_0t) \} \\
		&= E[U_1^2 sen(2\pi w_0(t+h))sen(2\pi w_0t)] + E[U_2^2 cos(2\pi w_0(t+h))cos(2\pi w_0t)] \\
		&= E[U_1^2]sen(2\pi w_0(t+h))sen(2\pi w_0t) + E[U_2^2]cos(2\pi w_0(t+h))cos(2\pi w_0t) \\
		&= \sigma^2 sen(2\pi w_0(t+h))sen(2\pi w_0t) + \sigma^2 cos(2\pi w_0(t+h))cos(2\pi w_0t) \\
		&= \sigma^2 [sen(2\pi w_0(t+h))sen(2\pi w_0t) + cos(2\pi w_0(t+h))cos(2\pi w_0t)] \\
		&= \sigma^2 cos(2\pi w_0 h)
\end{aligned}
$$

donde el termino central sea nula dado que $U_1$ y $U_2$ son independientes (tal que $E[U_1U_2] = E[U_1]E[U_2] = 0$) y la identidad trigonométrica para suma de ángulos para el coseno:

$$cos(\alpha-\beta) = cos(\alpha)cos(\beta)+sen(\alpha)sen(\beta)$$

**Problema 1.10** Supongamos que nos gustaría predecir una sola serie estacionaria $x_t$ con media cero y función de autocorrelación $\rho(h)$ en algún momento en el futuro, digamos, $t + \ell$, para $l > 0$.  
_a)_ Si predecimos usando solo $x_t$ y algún multiplicador de escala $A$, demuestre que el error de predicción cuadrático medio $MSE(A) = E[(x_{t+l} - Ax_t)^2]$ es minimizado por $A = \rho(l)$.  
_b)_ Demuestre que el error de predicción cuadrático medio mínimo es $MSE(A) = \gamma(0)[1 - \rho^2(l)]$.  
_c)_ Demuestre que si $x_{t+l} = Ax_t$, entonces $\rho(l) = 1$ si $A > 0$, y $ρ(l) = −1$ si $A < 0$.

De la definición de $MSE$, se tiene por producto notable que:

$$\begin{aligned}
MSE(A) &= E[(x_{t+l} − Ax_t)^2] \\
  &= E[x_{t+l}^2 - 2Ax_tx_{t+l} + A^2x_t^2] \\
	&= E[x_{t+l}^2] - 2AE[x_tx_{t+l}] + A^2E[x_t^2]
\end{aligned}
$$

Derivando con respecto a $A$, e igualando a cero queda:

$$-2E[x_tx_{t+l}] + 2AE[x_t^2] = 0$$

Resolviendo para $A$ da:

$$A = \frac{E[x_tx_{t+l}]}{E[x_t^2]}$$

El numerador es la autocovarianza $\gamma(l)$ ya que $x_t$ tiene media cero. El denominador es la autocovarianza $\gamma(0)$, por lo que $A = \gamma(l)/\gamma(0) = \rho(l)$ y queda demostrado.

Para demostrar el inciso _b)_, solo se necesita sacar $\gamma(0)$ como factor comun de la expansión del producto notable:

$$\begin{aligned}
MSE(A) &= E[(x_{t+l} − Ax_t)^2]  \\
	&= E[x_{t+l}^2] - 2AE[x_tx_{t+l}] + A^2E[x_t^2] \\
	&= \gamma(0) - 2\frac{\gamma(l)}{\gamma(0)}\gamma(l) + \left(\frac{\gamma(l)}{\gamma(0)}\right)^2\gamma(0) \\
	&= \gamma(0)\left(1 - 2\left(\frac{\gamma(l)}{\gamma(0)}\right)^2 + \left(\frac{\gamma(l)}{\gamma(0)}\right)^2\right) \\
	&= \gamma(0)\left(1 - \left(\frac{\gamma(l)}{\gamma(0)}\right)^2\right) \\
	&= \gamma(0)\left(1 - \rho^2(l)\right)
\end{aligned}
$$

De la definición se tiene:

$$\begin{aligned}
	\rho(l) &= \frac{\gamma(l)}{\gamma(0)} \\
		&= \frac{E[x_{t+l}x_t]}{E[x_t^2]} \\
		&= \frac{AE[x_t^2]}{E[x_t^2]} \\
		&= A
\end{aligned}
$$

Si $A > 0$, entonces $\rho(l) = 1$, dado que se esta correlación seria entre $x_t$ y ella misma. Si $A < 0$, la correlación es negativa, debido al signo de $A$, pero seria la minima posible ya que la correlación seria aun entre $x_t$ y ella misma, de forma que se escribe $\rho(l) = -1$

**Problema 1.11** Considere el proceso lineal definido en (1.31), como:

$$x_t = \mu + \sum_{j=-\infty}^\infty \psi_jw_{t-j}, \qquad \sum_{j=-\infty}^\infty \vert\psi_j\vert < \infty$$

_a)_ Verifique que la función de autocovarianza del proceso está dada por (1.32):  

$$\gamma_x(h) = \sigma_w^2 \sum_{j=-\infty}^\infty \psi_{j+h}\psi_j$$

Use el resultado para verificar su respuesta al Problema 1.7. _Pista_: Para $h \ge 0$, $cov(x_{t+h}, x_t) = cov(\sum_k \psi_k w_{t+h−k}, \sum_j \psi_jw_{t−j})$. Para cada $j \in \mathbb{Z}$, el único _superviviente_ será cuando $k = h + j$.  
_b)_ Demuestre que $x_t$ existe como un límite en el cuadrado medio (vea el Apéndice A).

Para el inciso _a)_ se escribe:

$$
\begin{aligned}
    \gamma(h) &= cov(x_t,x_{t+h}) = cov\left(\mu + \sum_{j=-\infty}^\infty \psi_jw_{t-j}, \mu + \sum_{j=-\infty}^\infty \psi_jw_{t+h-j}\right) \\
        &= E\left[\left(\sum_{j=-\infty}^\infty \psi_jw_{t-j}\right)\left(\sum_{j=-\infty}^\infty \psi_jw_{t+h-j}\right)\right] \\
        &= E\left[\sum_{j=-\infty}^\infty \sum_{k=-\infty}^\infty \psi_j\psi_kw_{t-j}w_{t+h-k} \right] \\
        &= \sum_{j=-\infty}^\infty \sum_{k=-\infty}^\infty \psi_j\psi_k E[w_{t-j}w_{t+h-k}] 
\end{aligned}
$$

Para cualesquiera valor de $j$ y $k$, los ruidos blancos se cancelan por ser independientes si $t-j \ne t+h-k$. Y solo los valores para los que $k=j+h$ no se cancelan sino que $E[w_{t-j}w_{t-j}] = \sigma_w^2$, de forma que:

$$\gamma_x(h) = \sigma_w^2 \sum_{j=-\infty}^\infty \psi_{j+h}\psi_j$$





**Problema 1.12** Para dos series débilmente estacionarias $x_t$ e $y_t$, verifique (1.30): $\rho_{xy}(h) = \rho_{yx}(−h)$.

Comprobar que $\rho_{xy}(h) = \rho_{yx}(−h)$, implica verificar que $\gamma_{xy}(h) = \gamma_{yx}(−h)$. Evidentemente:

$$
\begin{aligned}
    \gamma_{xy}(h) &= E[(x_t - \mu_x)(y_{t+h} - \mu_y)] \\
        &= E[(y_{t+h} - \mu_y)(x_t - \mu_x)] \\
        &= E[(y_{s} - \mu_y)(x_{s-h} - \mu_x)] \quad \text{usando }t = s-h \\
        &= \gamma_{yx}(-h)
\end{aligned}
$$

Luego, introduciendo esto en la definición de la función de correlación cruzada demuestra la igualdad buscada $\rho_{xy}(h) = \rho_{yx}(-h)$.


**Problema 1.13** Considere las dos series $x_t = w_t$ y $y_t = w_t − \theta w_{t−1} + u_t$, donde $w_t$ y $u_t$ son series de ruido blanco independientes con varianzas $\sigma_w^2$ y $\sigma_u^2$, respectivamente, y $\theta$ es una constante no especificada.  
_a)_ Exprese el ACF, $\rho_y(h)$, para $h = 0, \pm1, \pm2, \ldots$ de la serie $y_t$ en función de $\sigma_w^2$, $\sigma_u^2$ y $\theta$.  
_b)_ Determine el CCF, $\rho_{xy}(h)$ relacionando $x_t$ y $y_t$.  
_c)_ Demuestre que $x_t$ e $y_t$ son conjuntamente estacionarios.

Para el.inciso _a)_ se tiene que $\gamma_y(0) = 2\sigma_w^2 + \sigma_u^2$, de tal forma que para $h=0$, la ACF es igual a 1. Para $h=\pm1$, solo un elemento del ruido blanco $w_t$ no se cancela, y los $u_t$ se cancelan entre si, por lo que $\rho_y(1) = \sigma_w^2 / (2\sigma_w^2 + \sigma_u^2)$. Para $h\ge2$, la ACF es cero dado que los términos de $w$ y $u$ no comparten subscritos similares. Resumiendo:

$$\rho_y(h) = \begin{cases}
    1 & h =0, \\
    \frac{\sigma_w^2}{2\sigma_w^2 + \sigma_u^2} & h =1 \\ 
    0 & h \ge 2 
\end{cases}
$$

Para el inciso _b)_, se debe calcular primero el numerador de la CCF, es decir, calcular $\gamma_{xy}(h)$, para poder encontrar una expresión simplificada para la CCF, conociendo las varianzas $x_t$ y $y_t$. Para $h=0, \pm1$, hay un termino $w_t$ en $y_{t+h}$, por lo que este no se cancela y $\gamma_{xy}(h) = \sigma_w^2$. De resto, $\gamma_{xy}(h) = 0$ y, por lo tanto, se escribe: 

$$\rho_{xy}(h) = \begin{cases}
    \frac{\sigma_w}{\sqrt{2\sigma_w^2 + \sigma_u^2}} & h =0, \pm1 \\
    0 & h\ge 2
\end{cases}
$$

Como se ve arriba, la función de covarianza cruzada $\gamma_{xy}(h)$ depende solo del retraso $h$ y, por lo tanto, las series son conjuntamente estacionarias.

**Problema 1.14** Sea $x_t$ un proceso normal estacionario con media $\mu_x$ y función de autocovarianza $\gamma(h)$. Definir la serie de tiempo no lineal $y_t = exp\left{x_t\right}$.  
_a)_ Exprese la función media $E(y_t)$ en términos de $\mu_x$ y $\gamma(0)$. La función generadora de momentos de una variable aleatoria normal $x$ con media $\mu$ y varianza $\sigma^2$ es:

$$M_x(\lambda) = E[exp\left{\lambda x\right}] = exp\left{\mu\lambda + \frac{1}{2}\sigma^2\lambda^2\right}$$

_b)_ Determine la función de autocovarianza de $y_t$. La suma de las dos variables aleatorias normales $x_{t+h} + x_t$ sigue siendo una variable aleatoria normal.

Para el inciso _a)_, se nota que $E(y_t)=E[exp\left\{x_t\right\}]$, la cual es la función generadora de momentos $M_x(\lambda=1)$ para $\lambda=1$. De forma que:

$$E(y_t)= e^{\mu_x + \frac{1}{2}\sigma^2}$$

La función de autocovarianza es:

$$
\begin{aligned}
    \gamma_y(h) &= E[(y_t - \mu_y)(y_{t+h} - \mu_y)] \\
        &= E\left[\left(exp\{x_t\} - \left\{\mu + \frac{1}{2}\sigma^2\right\}\right)\left(exp\{x_{t+h}\} - \left\{\mu + \frac{1}{2}\sigma^2\right\}\right)\right] \\
        &= E\left[exp\{x_t + x_{t+h}\} - exp\left\{\mu + \frac{1}{2}\sigma^2\right\}^2\right] \\
        &= E\left[exp\{x_t + x_{t+h}\}\right] - exp\left\{\mu + \frac{1}{2}\sigma^2\right\}^2 \\
        &= E\left[exp\{x_t + x_{t+h}\}\right] - exp\left\{2\mu + \sigma^2\right\}
\end{aligned}
$$

Como $z_t = x_t + x_{t+h}$ es otra normal, entonces $E(z_t) = 2\mu_x$ y $var(z_t)=2\sigma^2$, por lo que:

$$
\begin{aligned}
	\gamma_y(h) &= E\left[exp\{x_t + x_{t+h}\}\right] - exp\left\{2\mu + \sigma^2\right\} \\
		&= exp\left\{2\mu + \sigma^2\right\} - exp\left\{2\mu + \sigma^2\right\} \\
		&= 0
\end{aligned}
$$

**Problema 1.15** Sea $w_t$, para $t = 0, \pm1, \pm2, \ldots$ un proceso de ruido blanco normal, y considerar la serie $x_t = w_t w_{t−1}$.
Determine la media y la función de autocovarianza de $x_t$ e indique si es estacionaria.

Dado que se trata de ruido blanco $E(x_t) = E(w_tw_{t-1}) = E(w_t)E(w_{t-1}) = 0$. La función de autocovarianza viene dada por $\gamma(h) = E(x_tx_{t+h}) = E(w_tw_{t-1}w_{t+h}w_{t+h-1}) = 0$, la cual no depende de $h$, por lo que la serie no es estacionaria.

**Problema 1.16** Considere la serie $x_t = sen(2\pi U_t)$, $t = 1, 2, \ldots$, donde $U$ tiene una distribución uniforme en el intervalo $(0, 1)$.  
_a)_ Demuestre que $x_t$ es débilmente estacionario.  
_b)_ Demuestre que $x_t$ no es estrictamente estacionario.  

La serie $x_t$ esta definida dentro del intervalo 0 a $2\pi$, dado que la distribución uniforme se define en el intervalo 0 a 1. Dentro del intervalo, la función seno recorre un periodo completo, por lo que su media es cero. Formalmente:

$$E[x_t] = \int_{-1}^{1} sen(2\pi U_t) dU_t = -cos(2\pi) + cos(-2\pi) = -cos(2\pi) + cos(2\pi) = 0$$

La función de autocovarianza viene dada por:

$$\begin{aligned}
	\gamma(h) &= E\left[x_tx_{t+h}\right] \\
		&= E\left[sen(2\pi U_t)sen(2\pi U_{t+h})\right]
\end{aligned}
$$

Cuando $h=0$, se tiene que $\gamma(h) = E\left[sen^2(2\pi U_t)\right] = \int_0^1 sen^2(2\pi U_t)dU_t = 1/2$ (la varianza de la distribución uniforme en el intervalo $(0,1)$ ). Cuando $h>0$, $\gamma(h) = 0$ dado que $U_t$ y $U_{t+h}$ son independientes entre si. Como la función media es constante y la función de autocovarianza depende del retraso solamente, y como la varianza es finita, entonces se demuestra que $x_t$ es débilmente estacionaria. 





**Problema 1.17** Supongamos que tenemos el proceso lineal $x_t$ generado por $x_t = w_t − \theta w_{t−1}$, $t = 0, 1, 2, \ldots$, donde $\{w_t\}$ es independiente e idénticamente distribuida con función característica $\phi_w()$, y $\theta$ es una constante fija. [Reemplazar _función característica_ con _función generadora de momento_ si se le indica que lo haga].  
_a)_ Exprese la función característica conjunta de $x_1, x_2, \ldots, x_n$, digamos, $\phi_{x_1,x_2,\ldots,x_n}(\lambda_1, \lambda_2, \ldots , \lambda_n)$, en términos de $\phi_w()$.  
_b)_ Deducir de _a)_ que $x_t$ es estrictamente estacionario.



<!---
**Problema 1.18** Suppose that xt is a linear process of the form (1.31). Prove
∞ 
h=−∞
|γ(h)| < ∞.

**Problema 1.19** Supongamos que xt = μ + wt + θwt−1, donde wt ∼ wn(0, σw2 ).
(a) Demuestre que la función media es E(xt) = μ.
(b) Demuestre que la función de autocovarianza de xt está dada por γx(0) = σw2 (1 + θ2),
γx(±1) = σw2 θ, y γx(h) = 0 en caso contrario.
(c) Demuestre que xt es estacionario para todos los valores de θ ∈ R.
(d) Use (1.35) para calcular var(x¯) para estimar μ cuando (i) θ = 1, (ii) θ = 0 y (iii) θ = −1
(e) En las series de tiempo, el tamaño de la muestra n suele ser grande, de modo que (n − n1) ≈ 1. Teniendo esto en cuenta, comente los resultados del inciso (d); en particular, ¿cómo cambia la precisión en la estimación de la media μ para los tres casos diferentes?

**Problema 1.20** (a) Simule una serie de n = 500 observaciones de ruido blanco gaussiano como en el ejemplo 1.8 y calcule el ACF de muestra, ˆ ρ(h), con un desfase de 20. Compare el ACF de muestra que obtenga con el ACF real, ρ(h). [Recuerde el Ejemplo 1.19.]
(b) Repita la parte (a) usando solo n = 50. ¿Cómo afecta el cambio de n a los resultados?

**Problema 1.21** (a) Simule una serie de n = 500 observaciones de promedio móvil como en el ejemplo 1.9 y calcule el ACF muestral, ˆ ρ(h), con un rezago de 20. Compare el ACF muestral que obtenga con el ACF real, ρ(h). [Recuerde el Ejemplo 1.20.]
(b) Repita la parte (a) usando solo n = 50. ¿Cómo afecta el cambio de n a los resultados?

**Problema 1.22** Aunque el modelo del problema 1.2(a) no es estacionario (¿por qué?), el ACF de muestra puede ser informativo. Para los datos que generó en ese problema, calcule y trace el ACF de muestra y luego comente.

**Problema 1.23** Simule una serie de n = 500 observaciones a partir del modelo de señal más ruido presentado en el Ejemplo 1.12 con σw2 = 1. Calcule el ACF de la muestra para retrasar 100 de los datos que generó y comente.

**Problema 1.24** Para la serie de tiempo yt descrita en el ejemplo 1.26, verifique el resultado establecido de que ρy(1) = −.47 y ρy(h) = 0 para h > 1.


**Problema 1.25** Una función de valor real g(t), definida en los números enteros, es definida no negativa si y solo si

aig(ti − tj)aj ≥ 0 

para todos los enteros positivos n y para todos los vectores a = (a1, a2, . . . , an)′ y t = (t1, t2, . . . , tn)′. Para la matriz G = {g(ti − tj); yo, j = 1, 2, . . . , n}, esto implica que a′Ga ≥ 0 para todos los vectores a. Se llama definido positivo si podemos reemplazar '≥' con '>' para todo un 0, el vector cero.
(a) Demuestre que γ(h), la función de autocovarianza de un proceso estacionario, es una función definida no negativa.
(b) Verifique que la autocovarianza muestral ˆ γ(h) es una función definida no negativa.

**Problema 1.26** Considere una colección de series de tiempo x1t, x2t, . . . , xNt que están observando alguna señal común μt observada en procesos de ruido e1t, e2t, . . . , eNt, con un modelo para la j-ésima serie observada dado por
xjt = μt + ejt.
Suponga que las series de ruido tienen medias cero y no están correlacionadas para diferentes j. Las funciones de autocovarianza comunes de todas las series vienen dadas por γe(s, t). Definir la media muestral
x¯t =
1 norte
Nj=1
xjt.
(a) Demuestre que E[x¯t] = μt.
(b) Demuestre que E[(x¯t − μ)2)] = N−1γe(t, t).
(c) ¿Cómo podemos usar los resultados para estimar la señal común?

**Problema 1.27** Un concepto utilizado en geoestadística, véase Journel y Huijbregts [109] o Cressie [45], es el de variograma, definido para un proceso espacial xs, s = (s1, s2), para s1, s2 = 0, ±1, ±2, . . ., como
Vx
(h) = 1
2
E[(xs+h − xs)2],
donde h = (h1, h2), para h1, h2 = 0, ±1, ±2, . . . Muestre que, para un proceso estacionario, las funciones de variograma y autocovarianza pueden relacionarse mediante Vx(h) = γ(0) − γ(h),
donde γ(h) es la función habitual de covarianza del retraso h y 0 = (0, 0). Tenga en cuenta la fácil extensión a cualquier dimensión espacial.

**Problema 1.28** Supongamos que xt = β0 + β1t, donde β0 y β1 son constantes. Demostrar como n → ∞, ρˆx(h) → 1 para h fija, donde ˆ ρx(h) es el ACF (1.37).

**Problema 1.29** a) Suponga que xt es una serie de tiempo débilmente estacionaria con media cero y con
función de autocovarianza absolutamente sumable, γ(h), tal que
∞
h=−∞
γ(h) = 0.
Demuestre que √n x¯ →p 0, donde ¯ x es la media muestral (1.34).
(b) Dé un ejemplo de un proceso que satisfaga las condiciones de la parte (a). ¿Qué tiene de especial este proceso?

**Problema 1.30** 

**Problema 1.31** 

**Problema 1.32** 

--->
