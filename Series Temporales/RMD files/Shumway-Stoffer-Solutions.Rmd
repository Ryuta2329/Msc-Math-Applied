---
title: Soluciones a problemas de _Time Series Analysis and its Applications_ de Shumway y Stoffer
author: Marcelo Molinatti
date: "`r Sys.Date()`"
output:
 html_document:
  number_sections: yes
  keep_md: yes
header-includes:
 - \usepackage{amsmath}
 - \usepackage{amsfonts}
lang: es
---

```{r setup}
library(astsa)
library(ggfortify)
library(kableExtra)
```

* Ejercicios del [capitulo 1](#capitulo-1). Características de series temporales.
* Ejercicios del [capitulo 2](#capitulo-2). Análisis de datos exploratorio.
* Ejercicios del [capitulo 3](#capitulo-3). Modelos ARIMA.

# Capitulo 1.

**Problema 1.1** Para comparar las señales de terremotos y explosiones, represente los datos en el mismo gráfico usando diferentes colores o diferentes tipos de líneas y comente los resultados.

```{r p1-1, label="fig:p1-1", fig.caption=""}
p1 <- autoplot(cbind(EQ5, EXP6), facets=FALSE) +
	xlab("Índice de la muestra") + ylab("Amplitud") +
	scale_colour_manual(name="", values=c("20","200")) + 
	theme_light() +
	theme(legend.position = c(0.3, 0.85))
```

Lo primero que sale a la vista en el gráfico es que la variabilidad de las señales de terremoto es mayor (la amplitud de la señal es mayor) y que esta variabilidad se extiende por más tiempo en el registro, mientras que la de explosiones es menor la variabilidad y la amplitud solo se extiende un intervalo de tiempo corto. 

**Problema 1.2** Considere un modelo de señal más ruido de la forma general $x_t = s_t + w_t$, donde $w_t$ es ruido blanco gaussiano con $\sigma^2_w = 1$. Simule y grafique $n = 200$ observaciones de cada uno de los siguientes dos modelos.

_a)_ $x_t = s_t + w_t$, para $t = 1, \ldots , 200$, donde:

$$
s_t = \begin{cases}
    0, & t= 1, \ldots, 100\\
    10\text{exp}{-\frac{(t-100)}{20}}\text{cos}(2\pi t/4), & t=101, \ldots, 200
  \end{cases}
  \label{eq:p1-2-st_a}
$$

_b)_ $x_t = s_t + w_t$, para $t = 1, \ldots , 200$, donde

$$
s_t = \begin{cases}
    0, & t= 1, \ldots, 100\\
    10\text{exp}{-\frac{(t-100)}{200}}\text{cos}(2\pi t/4), & t=101, \ldots, 200
  \end{cases}
  \label{eq:p1-2-st_b}
$$

```{r p1-2, label="fig:p1-2", fig.caption=""}
xt_a <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 20) * cos(2 * pi * 101:200 / 4)) + rnorm(200, 0, 1)
xt_b <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 200) * cos(2 * pi * 101:200 / 4)) + rnorm(200, 0, 1)
xt <- ts(cbind(xt_a, xt_b), start=1)

p2 <- autoplot(xt, facets=TRUE) +
	xlab("Índice de la muestra") + ylab("Amplitud") + 
	theme_light()

cowplot::plot_grid(p1, p2, nrow=1)
```

_c)_ Compare la apariencia general de las series _a)_ y _b)_ con la serie de terremoto y la serie de explosión. Además, traza (o dibuja)
y compare los moduladores de señal _a)_ $exp{−t/20}$ y _b)_ $exp{−t/200}$, para $t = 1, 2, \ldots, 100$.

La serie en _a)_ parece describir de forma adecuada la fase P y S de las explosiones, dada la caida rapida de la amplitud al inicio de la fase S, y tambien notando que la variabilidad es bastane similar es esta serie con la serie de explosiones. Sun embargo, en la fase P, la serie del inciso _a)_ no parece realizar un buen trabajo en simular la serie de explosiones, principalemnte al inicio donde parece haber un cambio de variabilidad importante.

En el caso de la serie en el inciso _b)_, la fase S parece ser similar tambien en amplitud, aunque la serie parece ser mas regular y menos variable que la registrada en Escandinavia para la serie de terremoto. De igual forma, en la fase P la serie no tiene una variabilidad tan grande haciendo que la similitud en esta zona sea distinta (a lo mejor, un ruido aleatorio de varianza 1 no es lo suficientemente bueno para simular el proceso registrado en Escandinavia).

```{r p1-2-modulators, label="fig:p1-2-2", fig.caption=""}
modulators <- data.frame(mod_a=exp(-(1:100) / 20), mod_b=exp(-(1:100) / 200))

autoplot(ts(modulators, start=1), facets=FALSE) +
	xlab("t") + ylab(latex2exp::TeX("$exp\\{-t/\\tau\\}$")) + 
	scale_colour_manual(name=latex2exp::TeX("\\tau"), values=c("20","200"), labels=c("20","200")) + 
	theme_light()
```

Al ver los moduladores de las señales, se puede verificar que la caida exponencial es mas rapida en la señal del inciso _a)_, lo cual explica la rapida desaparicion de la señal en un intervalo corto del tiempo, mientras que la caida mas lenta del modulador de la señal del inciso _b)_ explica la persintencai de la señal en un intervalo de tiempo amplio.

**Problema 1.3** _a)_ Generar $n = 100$ observaciones a partir de la autorregresión

$$x_t = -.9x_{t-2} + w_t$$

con $\sigma_w = 1$. A continuación, aplique el filtro de promedio móvil $v_t = (x_t + x_{t−1} + x_{t−2} + x_{t−3})/4$ a $x_t$. Ahora trace $x_t$ como una línea y superponga $v_t$ como una línea discontinua. Comente sobre el comportamiento de $x_t$ y cómo la aplicación del filtro de promedio móvil cambia ese comportamiento.  
_b)_ Repite _a)_ pero con $x_t = cos(2πt/4)$.  
_c)_ Repite _b)_ pero con ruido $N(0, 1)$, $x_t = cos(2πt/4) + w_t$.  
_d)_ Compare y contraste _a)_–_c)_; i.e., como cambia el promedio móvil cada serie. 

```{r p1-3, label="fig:p1-3", fig.caption="", warning=FALSE, out.width="100%"}
sims <- data.frame(
  # Iniciso a)
  a=stats::filter(rnorm(200, 0, 1), filter=c(0, -.9), method="recursive")[-(1:100)],
  # Iniciso b)
  b=cos(2 * pi * 1:100 / 4),
  # Iniciso c)
  c=cos(2 * pi * 1:100 / 4) + rnorm(200, 0, 1))

legend <- NULL
plot_list <- apply(sims, 2, function(ts) {
	# Aplico el filtrp sobre la serie:
	vt <- stats::filter(ts, filter=rep(1/4, 4), sides=1, method="convolution")

	# Grafico
	p1 <- autoplot(cbind(ts, vt), facets=FALSE) +
	  xlab("Índice") + ylab("") + 
	  aes(linetype = plot_group) +
	  scale_linetype_manual(name="", labels=c(expression("x"["t"]), expression("v"["t"])), values=c(1, 2)) +
  	scale_colour_manual(name="", values=c("20","1"), labels=c(expression("x"["t"]), expression("v"["t"]))) + 
	  theme_light()

	  legend <<- cowplot::get_legend(p1 + theme(legend.box.margin = margin(0, 0, 0, 1)))

	  p1
})

cowplot::plot_grid(
	cowplot::plot_grid(plotlist=lapply(plot_list, function(x) x + theme(legend.position="none")), 
		nrow=1, align = 'vh', labels = c("A", "B", "C"), hjust = -1), 
	legend, nrow = 1, rel_widths = c(3, .4))
```

Para el inciso _a)_, la serie antes del filtro muestra un cambio violento en el comportamiento de $x_t$, mientras que luego del filtro, la serie se suaviza y la variación de $v_t$ se ve menos pronunciada en amplitud (no se observan picos tan grandes).  
Para el inciso _b)_, la serie consiste de una función coseno regular, que luego de la suavización, se comporta como una constante, dado que el promediar elimina la variación dado que el periodo es de 4 unidades, y el promedio se hace con los 4 elementos inmediatamente en el pasado.  
Para el inciso _c)_, el ruido gaussiano agrega cierta variacion que quia la regularidad de la función coseno, por lo tanto la señal suavizada no se cancela en su totalidad, y esta muestra una variacion tambien, aunque su comportamiento es menos violeto. Sea como sea, aun se aprecia un póco el comportamiento de la onda coseno, tanto en $x_t$ como en $v_t$.  

Al comparar las tres series, podemos observar que el proceso de suavizar la serie cambia dependiendo de la forma de la señal subyacente. Cuando la señal varia entorno aun valor medio constante (como en el caso de la señal en A), el suavizado parece reducir la variabilidad en mayor medida que cuadno la media es variable (como en el caso de la señal en C).

**Problema 1.4** Demuestre que la función de autocovarianza se puede escribir como:

$$\gamma(s, t) = E[(x_s − \mu_s)(x_t − \mu_t)] = E(x_sx_t) − \mu_s\mu_t$$

Partiendo de la definición $E[(x_s − \mu_s)(x_t − \mu_t)]$, se distribuyen los terminos:

$$
\begin{aligned}
		\gamma(s, t) &= E[(x_s − \mu_s)(x_t − \mu_t)] \\
			&= E[x_sx_t − \mu_sx_t - x_s\mu_t + \mu_s\mu_t] \\
			&= E[x_sx_t] - \mu_sE[x_t] - \mu_tE[x_s] + \mu_s\mu_t \\
			&= E[x_sx_t] - \mu_s\mu_t - \mu_s\mu_t + \mu_s\mu_t \\
			&= E[x_sx_t] - \mu_s\mu_t
\end{aligned}
$$

**Problema 1.5** Para las dos series, $x_t$, en el Problema 1.2 _a)_ y _b)_:  
_a)_ Calcule y grafique las funciones medias $\mu_x(t)$, para $t = 1, . . . , 200$.
_b)_ Calcule las funciones de autocovarianza, $\gamma_x(s, t)$, para $s, t = 1, . . . , 200$.

Para ambos casos, la función media es:

$$
\begin{aligned}
E[x_t] &= E[s_t + w_t] \\
  &= E[s_t] + E[w_t] \\
  &= E[s_t] + 0 \\
  &= E[s_t]
\end{aligned}
$$

De forma que, para el inciso _a)_ $E[x_{t,a}]$ viene dada por \ref{eq:p1-2-st_a}, y para el inciso _b)_ $E[x_{t,b}]$ viene dada por \ref{eq:p1-2-st_b}. 

```{r p1-5, fig.caption=""}
xt_a <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 20) * cos(2 * pi * 101:200 / 4)) 
xt_b <- c(rep(0, 100), 10 * exp(-(101:200 - 100) / 200) * cos(2 * pi * 101:200 / 4)) 
xt <- ts(cbind(xt_a, xt_b), start=1)

autoplot(xt, facets=FALSE) +
	xlab("Índice de la muestra") + ylab(expression("E[x"["t"]*"]")) + 
	scale_colour_manual(name="", values=c("20","200"), labels=c("a", "b")) +
	theme_light()
```

La función de autocovarianza se puede encontrar usando el resultado del problema 1.4 y el resultado anterior:

$$
\begin{aligned}
	\gamma(s, t) &= E[x_tx_s] - \mu_t\mu_s \\
		  &= E[(s_t + w_t)(s_s + w_s)] - \mu_t^2 \\
		  &= E[s_ts_s + s_tw_s + s_sw_t + w_sw_t] - \mu_t^2 \\
		  &= E[s_ts_s] + s_tE[w_s] + s_sE[w_t] + E[w_sw_t] - \mu_t^2 \\
		  &= s_ts_s + 0 + 0 + E[w_s]E[w_t] - \mu_t^2 \\
		  &= s_ts_s - \mu_t^2
\end{aligned}
$$

**Problema 1.6** Considere la serie de tiempo: $x_t = \beta_1 + \beta_2t + w_t$, donde $\beta_1$ y $\beta_2$ son constantes conocidas y $w_t$ es un proceso de ruido blanco con varianza $\sigma_w^2$.

_a)_ Determine si $x_t$ es estacionario.
_b)_ Demuestre que el proceso $y_t = x_t − x_{t−1}$ es estacionario.
_c)_ Demuestre que la media del promedio móvil:

$$v_t = \frac{1}{2q + 1}\sum_{j=-q}^q x_{t-j}$$

es $\beta_1 + \beta_2t$, y dé una expresión simplificada para la función de autocovarianza.

Para el incisio _a)_, se puede demostrar que $x_t$ no es estacionario ya que $E[x_t] = E[\beta_1 + \beta_2t + w_t] = \beta_1 + \beta_2t + E[w_t] = \beta_1 + \beta_2t$ no es independiente del tiempo. Además, la función de autocovarianza es:

$$
\begin{aligned}
	\gamma(s, t) &= E[(x_{t} - \mu_t)(x_s - \mu_s)]\\
		&= E[(\beta_1 + \beta_2t + w_t - \beta_1 - \beta_2t)(\beta_1 + \beta_2s + w_s - \beta_1 - \beta_2s)] \\
		&= E[w_tw_s] 
\end{aligned}
$$

Cuando $s=t$, entonces $\gamma(s, t) = \sigma_w^2$. Si $s\ne t$, los ruidos aleatorios son independientes y por lo tanto $\gamma(s, t) = 0$. Esta no depdnede de las diferencias entre $s$ y $t$.  
Por otro lado, en el inciso _b)_ se tiene el proceso $y_t = x_t − x_{t−1}$, que se escribe en terminos de las variables $\beta$ como:

$$
\begin{aligned}
	y_t &= x_t − x_{t−1} \\
	  &= \beta_1 + \beta_2t + w_t - (\beta_1 + \beta_2(t-1) + w_{t-1}) \\
	  &= \beta_2 + (w_t - w_{t-1})
\end{aligned}
$$

cuya media es $E[y_t] = E[\beta_2 + (w_t - w_{t-1})] = \beta_2 + E[w_t - w_{t-1}] = \beta_2$. Su función de autocovarianza es:

$$
\begin{aligned}
	\gamma(s, t) &= E[(y_{t} - \mu_t)(y_s - \mu_s)]\\
		&= E[\beta_2 + (w_t - w_{t-1}) - \beta_2)(\beta_2 + (w_s - w_{s-1}) - \beta_2)] \\
		&= E[(w_t - w_{t-1})(w_s - w_{s-1})] 
\end{aligned}
$$

Cuando $s=t$, entonces $\gamma(s, t) = 2\sigma_w^2$. Cuando $s=t-1$, entonces $\gamma(s, t) = E[(w_t - w_{t-1})(w_{t-1} - w_{t-2})] = \sigma_w^2$; cuando $s \le t-2$, $\gamma(s, t) = 0$. Por lo que se puede escribir:

$$
gamma_y(s, t) = \begin{cases}
		2\sigma_w^2, & \vert h\vert = 0,
		\sigma_w^2, & \vert h \vert = 1,
		0, & \vert h \vert > 1 
	\end{cases}
$$

Por lo tanto el proceso $y_t$ es estacionario.

Para el promedio móvil del inciso _c)_, se tiene una media de:

$$
\begin{aligned}
	E[v_t] &= E[\frac{1}{2q + 1}\sum_{j=-q}^q x_{t-j}] \\
		&= \frac{1}{2q + 1}E\left[\sum_{j=-q}^q x_{t-j}\right] \\
		&= \frac{1}{2q + 1}E\left[\sum_{j=-q}^q \beta_1 + \beta_2(t-j) + w_{t-j}\right] \\
		&= \frac{1}{2q + 1}E\left[ (2q + 1)\beta_1 + (2q + 1)\beta_2t + \sum_{j=-q}^q w_{t-j}\right] \\
		&= \frac{1}{2q + 1}\left((2q + 1)\beta_1 + (2q + 1)\beta_2t + E\left[\sum_{j=-q}^q w_{t-j}\right] \right)\\
		&= \beta_1 + \beta_2t + \frac{1}{2q + 1}\sum_{j=-q}^q E[w_{t-j}] \\
		&= \beta_1 + \beta_2t
\end{aligned}
$$

Para la función de autocovarianza:

$$
\begin{aligned}
	\gamma_v(s, t) &= E[(v_{t} - \mu_t)(v_s - \mu_s)] \\
		&= E\left[\frac{1}{2q + 1}\sum_{j=-q}^q x_{t-j} - \beta_1-\beta_2t)(\frac{1}{2q + 1}\sum_{j=-q}^q x_{s-j} - \beta_1- \beta_2s)\right] \\
		&= \left(\frac{1}{2q + 1}\right)^2 E\left[\left( \sum_{j=-q}^q w_{t-j} \right)\left( \sum_{j=-q}^q w_{s-j} \right)\right]
\end{aligned}
$$

Cuando $s=t$, se tiene que $\gamma_v(s, t) = \frac{\sigma_w^2}{2q + 1}$. Si $s = t-1$, se tiene:

$$
\begin{aligned}
	E\left[\left( \sum_{j=-q}^q w_{t-j} \right)\left( \sum_{j=-q}^q w_{s-j} \right)\right] &= 
			E[(w_{t+q} + w_{t+q-1} + \ldots + w_{t-q+1} + w_{t-q})(w_{t+q-1} + w_{t+q-2} + \ldots + w_{t-q} + w_{t-q-1})] \\
		&= 2q\sigma_w^2
\end{aligned}
$$

Si $s=t-2$, se tiene que solo quedan $2q - 1$ terminos que no se anulan, y por lo tanto, $(2q - 1)\sigma_w^2$. De forma que se puede escribri la función de aitocovarianza en terminos de la diferencia $h=s-t$ como:

$$\gamma_v(s, t) = \left(\frac{2q + 1 - h}{(2q + 1)^2}\right)\sigma_w^2$$

lo cual muestra que la funcion de autocovarianza va decreciendo con la diferencia de tiempo $h$.

**Problema 1.7** Para un proceso de promedio móvil de la forma $x_t = w_{t−1} + 2w_t + w_{t+1}$, donde $w_t$ son independientes con medias cero y varianza $\sigma_w^2$, determine las funciones de autocovarianza y autocorrelación en función del desfase $h = s − t$ y grafique el ACF
en función de $h$.

La función de autocovarianza es (dado que la media es 0, ya que se trata de la suma de la media de tres ruidos blancos): $\gamma(t,s) = E[(w_{t−1} + 2w_t + w_{t+1})(w_{s−1} + 2w_s + w_{s+1})]$. Cuando $s=t$, se tiene:

$$
\begin{aligned}
	\gamma(t,t) &= E[(w_{t−1} + 2w_t + w_{t+1})(w_{t−1} + 2w_t + w_{t+1})] \\
		&= cov(w_{t-1}, w_{t-1}) + 2 cov(w_{t}, w_{t}) + cov(w_{t+1}, w_{t+1}) \\
		&= 4\sigma_w^2
\end{alogned}
$$

Cuando la diferencia entre $s$ y $t$ es 1, solo los terminos del centro y la derecha no se anulan, por lo que $\gamma(t,t\pm1) = 3\sigma_w^2$. Si la diferencia es de 2 solo un termino (el de la izquierda) no se cancela, por lo que $\gamma(t,t\pm2) = \sigma_w^2$. Para diferencias mayores que 2, la función de autocovarianza es cero. Queda entonces:

$$\gamma(t,s) = \begin{cases}
	4\sigma_w^2, & h=0 \\
	3\sigma_w^2, & h=\pm1 \\
	\sigma_w^2, & h=\pm2 \\
	0, & h=\pm3, \pm4, \ldots
\end{cases}$$

La función de autocorrelación queda:

$$\rho(t,s) = \begin{cases}
	1, & h=0 \\
	3/4, & h=\pm1 \\
	1/4, & h=\pm2 \\
	0, & h=\pm3, \pm4, \ldots
\end{cases}$$

El grafico de esta es:

```{r ex09, label="fig:p1-7"}
df <- data.frame(lag=-5:5, rho=c(0, 0, 0, .25, .75, 1, .75, .25, 0, 0, 0))

ggplot(data = df, mapping = aes(x = lag, y = rho)) +
  geom_point() +
  geom_hline(aes(yintercept = 0)) +
  geom_segment(mapping = aes(xend = lag, yend = 0)) +
  ylab(latex2exp::TeX("$\\rho(s, t)$")) +
  theme_light()
```

**Problema 1.8** Considere el modelo de paseo aleatorio con deriva $x_t = \delta + x_{t−1} + w_t$, para $t = 1, 2, \ldots$, con $x_0 = 0$, donde $w_t$ es ruido blanco con varianza $\sigma_w^2$.  
_a)_ Demuestre que el modelo se puede escribir como $x_t = \delta t + \sum_{k=1}^t w_k$.  
_b)_ Encuentre la función media y la función de autocovarianza de $x_t$.  
_c)_ Argumente que $x_t$ no es estacionario.  
_d)_ Muestre $\rho_x(t − 1, t) = \sqrt{\frac{t−1}{t}} \rightarrow 1$ cuando $t \rigtharrow \infty$. ¿Cuál es la implicación de este resultado?  
_e)_ Sugiera una transformación para hacer que la serie sea estacionaria y demuestre que la serie transformada es estacionaria.

Para el inciso _a)_, se puede rescribir el modelo usando la definicion del modelo de paseo alaetorio de forma recursiva, notando que en $t=0$ no hay una opbaservación, por lo que definimos $x_0=0$, quedando:

$$
\begin{aligned}
	x_t &= \delta + x_{t−1} + w_t \\
		&= \delta + (\delta + x_{t-2} + w_{t-1}) + w_t = 2\delta + x_{t-2} + w_t + w_{t-1} \\
		&= 2\delta + (\delta + x_{t-3} + w_{t-2}) + w_t + w_{t-1} = 3\delta + x_{t-3} + w_t + w_{t-1} + w_{t-2} \\
		&= \ldots \\
		&= (t-1)\delta + x_1 + \sum_{j=2}^t w_j = (t-1)\delta + (\delta + x_0 + w_1) + \sum_{j=2}^t w_j \\
		&= t\delta + \sum_{j=1}^t w_j
\end{aligned}
$$

lo cual demuestra la equivalencia.  
La función media del proceso es:

$$E[x_t] = E\left[t\delta + \sum_{j=1}^t w_j\right] = t\delta + E\left[\sum_{j=1}^t w_j\right] = t\delta + \sum_{j=1}^t E[w_j] = t\delta$$

y la función de autocovarianza:

$$
\begin{aligned}
	\gamma(t,s) &= E[(x_t - \mu_t)(x_s - \mu_s)] \\
		&= E\left[\left(t\delta + \sum_{j=1}^t w_j - t\delta\right)\left(s\delta + \sum_{j=1}^s w_j - s\delta\right)\right] \\
		&= E\left[\left(\sum_{j=1}^t w_j\right)\left(\sum_{j=1}^s w_j\right)\right] \\
		&= \text{min}\{s, t\} \sigma_w^2
\end{aligned}
$$

Como es posible ver de las funciones de media y autocovarianza, el proceso no es estacioanrio dado que la media depende del valor de $t$ y crece a medida que avamca el tiempo, sin cota alguna. Meintras que la función de autocvarianza no depende de la diferencia entre $s$ y $t$, sino del mínimo valor de estos. Por lo tanto, no es posible cumplir los requisitos de estacionaridad débil, y mucho menos al estacioanridad estricta.  
Se tiene que $\rho_x(t − 1, t) = \sqrt{\frac{t−1}{t}}$. Cuando $t \rightarrow \infty$, el numerador en la ACF tiende a $t$, haceindo que $\rho_x(t-1, t) \rightarrow 1$, lo cual implica una correlación perfecta entre el valor observado en $t$ y el inmediatamente adyacente, indicando en teoría, que sería posible el predecir con certeza el valor en $t$, conociendo el valor inmediatamente anterior.  
Una transformación posible es $y_t = x_t - x_{t-1}$, de tal forma que el modelo queda como $y_t=\delta + w_t$, cuya función media es $E[y_t] = E[\delta + w_t] = \delta$, una constante; y la función de autocovarianza es $\gamma_y(t,s) = E[(\delta + w_t - \delta)(\delta + w_s - \delta)] = E[(w_t)(w_s)]$ la cual es $\sigma_w^2$ cuando $h=t-s=0$, y 0 de otra forma. 

**Problema 1.9** Una serie de tiempo con un componente periódico se puede construir a partir de $x_t = U_1 sen(2\pi w_0t) + U_2 cos(2\pi w_0t)$, donde $U_1$ y $U_2$ son variables aleatorias independientes con media cero y $E(U_1^2) = E(U_2^2) = \sigma^2$. La constante $w_0$ determina el período o tiempo que tarda el proceso en realizar un ciclo completo. Demostrar que esta serie es débilmente estacionaria con función de autocovarianza:

$$\gamma(h) = \sigma^2cos(2\pi w_0 h)$$

Se tiene que la función media es:

$$
\begin{aligned}
	E[x_t] &= E[U_1 sen(2\pi w_0t) + U_2 cos(2\pi w_0t)] \\
		&= E[U_1 sen(2\pi w_0t)] + E[U_2 cos(2\pi w_0t)] \\
		&= E[U_1] sen(2\pi w_0t) + E[U_2] cos(2\pi w_0t) \\
		&= 0 + 0 = 0
\end{aligned}
$$

lo cual muestra que la media es constante e independiente de $t$. La función de autocovarianza es:

$$
\begin{aligned}
	\gamma(h) &= E[(x_{t+h} - E[x_t])(x_{t} - E[x_{t}])] \\
		&= E[(U_1 sen(2\pi w_0(t+h)) + U_2 cos(2\pi w_0(t+h)))(U_1 sen(2\pi w_0t) + U_2 cos(2\pi w_0t))] \\
		&= E\{U_1^2 sen(2\pi w_0(t+h))sen(2\pi w_0t) + U_1U_2 [sen(2\pi w_0(t+h))cos(2\pi w_0t) + sen(2\pi w_0t)cos(2\pi w_0(t+h))] + U_2^2 cos(2\pi w_0(t+h))cos(2\pi w_0t) \} \\
		&= E[U_1^2 sen(2\pi w_0(t+h))sen(2\pi w_0t)] + E[U_2^2 cos(2\pi w_0(t+h))cos(2\pi w_0t)] \\
		&= E[U_1^2]sen(2\pi w_0(t+h))sen(2\pi w_0t) + E[U_2^2]cos(2\pi w_0(t+h))cos(2\pi w_0t) \\
		&= \sigma^2 sen(2\pi w_0(t+h))sen(2\pi w_0t) + \sigma^2 cos(2\pi w_0(t+h))cos(2\pi w_0t) \\
		&= \sigma^2 [sen(2\pi w_0(t+h))sen(2\pi w_0t) + cos(2\pi w_0(t+h))cos(2\pi w_0t)] \\
		&= \sigma^2 cos(2\pi w_0 h)
\end{aligned}
$$

donde el termino central sea nula dado que $U_1$ y $U_2$ son independientes (tal que $E[U_1U_2] = E[U_1]E[U_2] = 0$) y la identidad trigonometrica para suma de angulos para el coseno ($cos(\alpha - \beta) = cos(\alpha)cos(\beta) + sen(\alpha)sen(\beta)$).

**Problema 1.10** Supongamos que nos gustaría predecir una sola serie estacionaria $x_t$ con media cero y función de autocorrelación $\rho(h)$ en algún momento en el futuro, digamos, $t + \ell$, para $l > 0$.  
_a)_ Si predecimos usando solo $x_t$ y algún multiplicador de escala $A$, demuestre que el error de predicción cuadrático medio $MSE(A) = E[(x_{t+l} - Ax_t)^2]$ es minimizado por $A = \rho(l)$.  
_b)_ Demuestre que el error de predicción cuadrático medio mínimo es $MSE(A) = \gamma(0)[1 - \rho^2(l)]$.  
_c)_ Demuestre que si $x_{t+l} = Ax_t$, entonces $\rho(l) = 1$ si $A > 0$, y $ρ(l) = −1$ si $A < 0$.

De la definicion de $MSE$, se tiene por producto notable que:

$$\begin{aligned}
MSE(A) = E[(x_{t+l} − Ax_t)^2] &= E[x_{t+l}^2 - 2Ax_tx_{t+l} + A^2x_t^2] \\
	&= E[x_{t+l}^2] - 2AE[x_tx_{t+l}] + A^2E[x_t^2]
\end{aligned}
$$

Derivando con respecto a $A$, e igualando a cero queda:

$$-2E[x_tx_{t+l}] + 2AE[x_t^2] = 0$$

Resolviendo para $A$ da:

$$A = \frac{E[x_tx_{t+l}]}{E[x_t^2]}$$

El numerador es la autocovarianza $\gamma(l)$ ya que $x_t$ tiene media cero. El denominador es la autocovarianza $\gamma(0)$, por lo que $A = \gamma(l)/\gamma(0) = \rho(l)$ y queda demostrado.

Para demostrar el inciso _b)_, solo se necesita sacar $\gamma(0)$ como factor comun de la expansion del producto notable:

$$\begin{aligned}
MSE(A) = E[(x_{t+l} − Ax_t)^2]  \\
	&= E[x_{t+l}^2] - 2AE[x_tx_{t+l}] + A^2E[x_t^2] \\
	&= \gamma(0) - 2\frac{\gamma(l)}{\gamma(0)}\gamma(l) + \left(\frac{\gamma(l)}{\gamma(0)}\right)^2\gamma(0) \\
	&= \gamma(0)\left(1 - 2\left(\frac{\gamma(l)}{\gamma(0)}\right)^2 + \left(\frac{\gamma(l)}{\gamma(0)}\right)^2\right) \\
	&= \gamma(0)\left(1 - \left(\frac{\gamma(l)}{\gamma(0)}\right)^2\right) \\
	&= \gamma(0)\left(1 - \rho^2(l)\right)
\end{aligned}
$$

De la definicion se tiene:

$$\begin{aligned}
	\rho(l) &= \frac{\gamma(l)}{\gamma(0)} \\
		&= \frac{E[x_{t+l}x_t]}{E[x_t^2]} \\
		&= \frac{AE[x_t^2]}{E[x_t^2]} \\
		&= A
\end{aligned}
$$

Si $A > 0$, entonces $\rho(l) = 1$, dado que se esta correlacion seria entre $x_t$ y ella misma. Si $A < 0$, la correlacion es negativa, debido al sogno de $A$, pero seria la minima posible ya que la correlacion seria aun entre $x_t$ y ella misma, de forma que se escribe $\rho(l) = -1$

**Problema 1.11** Considere el proceso lineal definido en (1.31), como:

$$x_t = \mu + \sum_{j=-\infty}^\infty \psi_jw_{t-j}, \qquad \sum_{j=-\infty}^\infty \vert\psi_j\vert < \infty$$

_a)_ Verifique que la función de autocovarianza del proceso está dada por (1.32):  

$$\gamma_x(h) = \sigma_w^2 \sum_{j=-\infty}^\infty \psi_{j+h}\psi_j$$

Use el resultado para verificar su respuesta al Problema 1.7. _Pista_: Para $h \ge 0$, $cov(x_{t+h}, x_t) = cov(\sum_k \psi_k w_{t+h−k}, \sum_j \psi_jw_{t−j})$. Para cada $j \in \mathbb{Z}$, el único _superviviente_ será cuando $k = h + j$.  
_b)_ Demuestre que $x_t$ existe como un límite en el cuadrado medio (vea el Apéndice A).

Para el inciso _a)_ se escribe:

$$
\begin{aligned}
    \gamma(h) &= cov(x_t,x_{t+h}) = cov\left(\mu + \sum_{j=-\infty}^\infty \psi_jw_{t-j}, \mu + \sum_{j=-\infty}^\infty \psi_jw_{t+h-j}\right) \\
        &= E\left[\left(\sum_{j=-\infty}^\infty \psi_jw_{t-j}\right)\left(\sum_{j=-\infty}^\infty \psi_jw_{t+h-j}\right)\right] \\
        &= E\left[\sum_{j=-\infty}^\infty \sum_{k=-\infty}^\infty \psi_j\psi_kw_{t-j}w_{t+h-k} \right] \\
        &= \sum_{j=-\infty}^\infty \sum_{k=-\infty}^\infty \psi_j\psi_k E[w_{t-j}w_{t+h-k}] 
\end{aligned}
$$

Para cualesquiera valor de $j$ y $k$, los ruidos blancos se cancelan por ser independientes si $t-j \ne t+h-k$. Y solo los valores para los que $k=j+h$ no se cancelan sino que $E[w_{t-j}w_{t-j}] = \sigma_w^2$, de forma que:

$$\gamma_x(h) = \sigma_w^2 \sum_{j=-\infty}^\infty \psi_{j+h}\psi_j$$





**Problema 1.12** Para dos series débilmente estacionarias $x_t$ e $y_t$, verifique (1.30): $\rho_{xy}(h) = \rho_{yx}(−h)$.

Comprobar que $\rho_{xy}(h) = \rho_{yx}(−h)$, implica verificar que $\gamma_{xy}(h) = \gamma_{yx}(−h)$. Evidentemente:

$$
\begin{aligned}
    \gamma_{xy}(h) &= E[(x_t - \mu_x)(y_{t+h} - \mu_y)] \\
        &= E[(y_{t+h} - \mu_y)(x_t - \mu_x)] \\
        &= E[(y_{s} - \mu_y)(x_{s-h} - \mu_x)] \quad \text{usando }t = s-h \\
        &= \gamma_{yx}(-h)
\end{aligned}
$$

Luego, introduciendo esto en la.definicion de la funcion de correlacion cruzada demuestra la igualdad buscada $\rho_{xy}(h) = \rho_{yx}(-h)$.


**Problema 1.13** Considere las dos series $x_t = w_t$ y $y_t = w_t − \theta w_{t−1} + u_t$, donde $w_t$ y $u_t$ son series de ruido blanco independientes con varianzas $\sigma_w^2$ y $\sigma_u^2$, respectivamente, y $\theta$ es una constante no especificada.  
_a)_ Exprese el ACF, $\rho_y(h)$, para $h = 0, \pm1, \pm2, \ldots$ de la serie $y_t$ en función de $\sigma_w^2$, $\sigma_u^2$ y $\theta$.  
_b)_ Determine el CCF, $\rho_{xy}(h)$ relacionando $x_t$ y $y_t$.  
_c)_ Demuestre que $x_t$ e $y_t$ son conjuntamente estacionarios.

Para el.inciso _a)_ se tiene que $\gamma_y(0) = 2\sigma_w^2 + \sigma_u^2$, de tal forma que para $h=0$, la ACF es igual a 1. Para $h=\pm1$, solo un elemento del ruido blanco $w_t$ no se cancela, y los $u_t$ se cancelan entre si, por lo que $\rho_y(1) = \sigma_w^2 / (2\sigma_w^2 + \sigma_u^2)$. Para $h\ge2$, la ACF es cero dado que los terminos de $w$ y $u$ no comparten subscritos similares. Resumiendo:

$$\rho_y(h) = \begin{cases}
    1 & h =0, \\
    \frac{\sigma_w^2}{2\sigma_w^2 + \sigma_u^2} & h =1 \\ 
    0 & h \ge 2 
\end{cases}
$$

Para el inciso _b)_, se debe calcular primero el numerador de la CCF, es decir, calcular $\gamma_{xy}(h)$, para poder encontrar una expresion simplificada para la CCF, conociendo las varianzas $x_t$ y $y_t$. Para $h=0, \pm1$, hay un termino $w_t$ en $y_{t+h}$, por lo que este no se cancela y $\gamma_{xy}(h) = \sigma_w^2$. De resto, $\gamma_{xy}(h) = 0$ y, por lo tanto, se escribe: 

$$\rho_{xy}(h) = \begin{cases}
    \frac{\sigma_w}{\sqrt{2\sigma_w^2 + \sigma_u^2}} & h =0, \pm1 \\
    0 & h\ge 2
\end{cases}
$$

Como se ve arriba, la funcion de covarianza cruzada $\gamma_{xy}(h)$ depende solo del retraso $h$ y, por lo tanto, las series son conjuntamente estacionarias.

**Problema 1.14** Sea $x_t$ un proceso normal estacionario con media $\mu_x$ y función de autocovarianza $\gamma(h)$. Definir la serie de tiempo no lineal $y_t = exp{x_t}$.  
_a)_ Exprese la función media $E(y_t)$ en términos de $\mu_x$ y $\gamma(0)$. La función generadora de momentos de una variable aleatoria normal $x$ con media $\mu$ y varianza $\sigma^2$ es:

$$M_x(\lambda) = E[exp{\lambda x}] = exp\left{\mu\lambda + \frac{1}{2}\sigma^2\lambda^2\right}$$

_b)_ Determine la función de autocovarianza de $y_t$. La suma de las dos variables aleatorias normales $x_{t+h} + x_t$ sigue siendo una variable aleatoria normal.

Para el inciso _a)_, se nota que $E(y_t)=E[exp{x_t}]$, la cual es la función generadora de momentos $M_x(\lambda=1)$ para $\lambda=1$. De forma que:

$$E(y_t)= exp\left{\mu_x + \frac{1}{2}\sigma^2\right}$$

La funcion de autocovarianza es:

$$
\begin{aligned}
    \gamma_y(h) &= E[(y_t - \mu_y)(y_{t+h} - \mu_y)] \\
        &= E\left[\left(exp{x_t} - \left{\mu + \frac{1}{2}\sigma^2\right}\right)\left(exp{x_{t+h}} - \left{\mu + \frac{1}{2}\sigma^2\right}\right)\right] \\
        &= E\left[exp{x_t + x_{t+h}} - exp\left{\mu + \frac{1}{2}\sigma^2\right}^2\right] \\
        &= E\left[exp{x_t + x_{t+h}}\right] - exp\left{\mu + \frac{1}{2}\sigma^2\right}^2 \\
        &= E\left[exp{x_t + x_{t+h}}\right] - exp\left{2\mu + \sigma^2\right}
\end{aligned}
$$

Como $z_t = x_t + x_{t+h}$ es otra normal, entonces $E(z_t) = 2\mu_x$ y $var(z_t)=2\sigma^2$, por lo que:

$$
\begin{aligned}
	\gamma_y(h) &= E\left[exp{x_t + x_{t+h}}\right] - exp\left{2\mu + \sigma^2\right} \\
		&= exp\left{2\mu + \sigma^2\right} - exp\left{2\mu + \sigma^2\right} \\
		&= 0
\end{aligned}
$$

**Problema 1.15** Sea $w_t$, para $t = 0, \pm1, \pm2, \ldots$ un proceso de ruido blanco normal, y considerar la serie $x_t = w_t w_{t−1}$.
Determine la media y la función de autocovarianza de $x_t$ e indique si es estacionaria.

Dado que se trata de ruido blanco $E(x_t) = E(w_tw_{t-1}) = E(w_t)E(w_{t-1}) = 0$. La función de autocovarianza viene dada por $\gamma(h) = E(x_tx_{t+h}) = E(w_tw_{t-1}w_{t+h}w_{t+h-1}) = 0$, la cual no depende de $h$, por lo que la serie no es estacionaria.

**Problema 1.16** Considere la serie $x_t = sen(2\pi U_t)$, $t = 1, 2, \ldots$, donde $U$ tiene una distribución uniforme en el intervalo $(0, 1)$.  
_a)_ Demuestre que $x_t$ es débilmente estacionario.  
_b)_ Demuestre que $x_t$ no es estrictamente estacionario.  

La serie $x_t$ esta definida dentro del intervalo 0 a $2\pi$, dado que la distribución uniforme se define en el intervalo 0 a 1. Dentro del intervalo, la función seno recorre un periodo completo, por lo que su media es cero. Formalmente:

$$E[x_t] = \int_{-1}^{1} sen(2\pi U_t) dU_t = -cos(2\pi) + cos(-2\pi) = -cos(2\pi) + cos(2\pi) = 0$$

La función de autocovarianza viene dada por:

$$\begin{aligned}
	\gamma(h) &= E\left[x_tx_{t+h}\right] \\
		&= E\left[sen(2\pi U_t)sen(2\pi U_{t+h})\right]
\end{aligned}
$$

Cuando $h=0$, se tiene que $\gamma(h) = E\left[sen^2(2\pi U_t)\right] = \int_0^1 sen^2(2\pi U_t)dU_t = 1/2$ (la varianza de la distribución uniforme en el intervalo $(0, 1)$). Cuando $h>0$, $\gamma(h) = 0$ dado que $U_t$ y $U_{t+h}$ son independientes entre si. Como la funcion media es constante y la funcion de autocovarianza depende del retraso solamente, y como la varianza es finita, entonces sde demuestra que $x_t$ es debilmente estacionaria. 





**Problema 1.17** Supongamos que tenemos el proceso lineal $x_t$ generado por $x_t = w_t − \theta w_{t−1}$, $t = 0, 1, 2, \ldpts$, donde ${wt }$ es independiente e idénticamente distribuida con función característica $\phi w(\dot)$, y $\theta$ es una constante fija. [Reemplazar _función característica_ con _función generadora de momento_ si se le indica que lo haga].  
_a)_ Exprese la función característica conjunta de $x_1, x_2, \ldots, x_n$, digamos, $\phi_{x_1,x_2,\ldots,x_n}(\lambda_1, \lambda_2, \ldots , \lambda_n)$, en términos de $\phi w(\dot)$.  
_b)_ Deducir de _a)_ que $x_t$ es estrictamente estacionario.



<!---
**Problema 1.18** Suppose that xt is a linear process of the form (1.31). Prove
∞ 
h=−∞
|γ(h)| < ∞.

**Problema 1.19** Supongamos que xt = μ + wt + θwt−1, donde wt ∼ wn(0, σw2 ).
(a) Demuestre que la función media es E(xt) = μ.
(b) Demuestre que la función de autocovarianza de xt está dada por γx(0) = σw2 (1 + θ2),
γx(±1) = σw2 θ, y γx(h) = 0 en caso contrario.
(c) Demuestre que xt es estacionario para todos los valores de θ ∈ R.
(d) Use (1.35) para calcular var(x¯) para estimar μ cuando (i) θ = 1, (ii) θ = 0 y (iii) θ = −1
(e) En las series de tiempo, el tamaño de la muestra n suele ser grande, de modo que (n − n1) ≈ 1. Teniendo esto en cuenta, comente los resultados del inciso (d); en particular, ¿cómo cambia la precisión en la estimación de la media μ para los tres casos diferentes?

**Problema 1.20** (a) Simule una serie de n = 500 observaciones de ruido blanco gaussiano como en el ejemplo 1.8 y calcule el ACF de muestra, ˆ ρ(h), con un desfase de 20. Compare el ACF de muestra que obtenga con el ACF real, ρ(h). [Recuerde el Ejemplo 1.19.]
(b) Repita la parte (a) usando solo n = 50. ¿Cómo afecta el cambio de n a los resultados?

**Problema 1.21** (a) Simule una serie de n = 500 observaciones de promedio móvil como en el ejemplo 1.9 y calcule el ACF muestral, ˆ ρ(h), con un rezago de 20. Compare el ACF muestral que obtenga con el ACF real, ρ(h). [Recuerde el Ejemplo 1.20.]
(b) Repita la parte (a) usando solo n = 50. ¿Cómo afecta el cambio de n a los resultados?

**Problema 1.22** Aunque el modelo del problema 1.2(a) no es estacionario (¿por qué?), el ACF de muestra puede ser informativo. Para los datos que generó en ese problema, calcule y trace el ACF de muestra y luego comente.

**Problema 1.23** Simule una serie de n = 500 observaciones a partir del modelo de señal más ruido presentado en el Ejemplo 1.12 con σw2 = 1. Calcule el ACF de la muestra para retrasar 100 de los datos que generó y comente.

**Problema 1.24** Para la serie de tiempo yt descrita en el ejemplo 1.26, verifique el resultado establecido de que ρy(1) = −.47 y ρy(h) = 0 para h > 1.


**Problema 1.25** Una función de valor real g(t), definida en los números enteros, es definida no negativa si y solo si

aig(ti − tj)aj ≥ 0 

para todos los enteros positivos n y para todos los vectores a = (a1, a2, . . . , an)′ y t = (t1, t2, . . . , tn)′. Para la matriz G = {g(ti − tj); yo, j = 1, 2, . . . , n}, esto implica que a′Ga ≥ 0 para todos los vectores a. Se llama definido positivo si podemos reemplazar '≥' con '>' para todo un 0, el vector cero.
(a) Demuestre que γ(h), la función de autocovarianza de un proceso estacionario, es una función definida no negativa.
(b) Verifique que la autocovarianza muestral ˆ γ(h) es una función definida no negativa.

**Problema 1.26** Considere una colección de series de tiempo x1t, x2t, . . . , xNt que están observando alguna señal común μt observada en procesos de ruido e1t, e2t, . . . , eNt, con un modelo para la j-ésima serie observada dado por
xjt = μt + ejt.
Suponga que las series de ruido tienen medias cero y no están correlacionadas para diferentes j. Las funciones de autocovarianza comunes de todas las series vienen dadas por γe(s, t). Definir la media muestral
x¯t =
1 norte
Nj=1
xjt.
(a) Demuestre que E[x¯t] = μt.
(b) Demuestre que E[(x¯t − μ)2)] = N−1γe(t, t).
(c) ¿Cómo podemos usar los resultados para estimar la señal común?

**Problema 1.27** Un concepto utilizado en geoestadística, véase Journel y Huijbregts [109] o Cressie [45], es el de variograma, definido para un proceso espacial xs, s = (s1, s2), para s1, s2 = 0, ±1, ±2, . . ., como
Vx
(h) = 1
2
E[(xs+h − xs)2],
donde h = (h1, h2), para h1, h2 = 0, ±1, ±2, . . . Muestre que, para un proceso estacionario, las funciones de variograma y autocovarianza pueden relacionarse mediante Vx(h) = γ(0) − γ(h),
donde γ(h) es la función habitual de covarianza del retraso h y 0 = (0, 0). Tenga en cuenta la fácil extensión a cualquier dimensión espacial.

**Problema 1.28** Supongamos que xt = β0 + β1t, donde β0 y β1 son constantes. Demostrar como n → ∞, ρˆx(h) → 1 para h fija, donde ˆ ρx(h) es el ACF (1.37).

**Problema 1.29** a) Suponga que xt es una serie de tiempo débilmente estacionaria con media cero y con
función de autocovarianza absolutamente sumable, γ(h), tal que
∞
h=−∞
γ(h) = 0.
Demuestre que √n x¯ →p 0, donde ¯ x es la media muestral (1.34).
(b) Dé un ejemplo de un proceso que satisfaga las condiciones de la parte (a). ¿Qué tiene de especial este proceso?

**Problema 1.30** 

**Problema 1.31** 

**Problema 1.32** 

--->

# Capitulo 2.

**Problema 2.1** Un modelo estructural para los datos de Johnson y Johnson, digamos $y_t$, sea $x_t = log(y_t)$. En este problema, vamos a ajustar un tipo especial de modelo estructural, $x_t = T_t + S_t + N_t$ donde $T_t$ es un componente de tendencia, $S_t$ es un componente estacional y $N_t$ es ruido. En nuestro caso, el tiempo $t$ está en trimestres ($1960{,}00, 1960{,}25, \ldots$) por lo que una unidad de tiempo es un año.   
_a)_ Ajuste el modelo de regresión

$$x_t = \beta t + \alpha_1 Q_1(t) + \alpha_2 Q_2(t) + \alpha_3 Q_3(t) + \alpha_4 Q_4(t) + w_t$$

donde $Q_i(t) = 1$ si el tiempo $t$ corresponde al trimestre $i = 1, 2, 3, 4$ y cero en caso contrario. Las $Q_i(t)$ se denominan variables indicadoras. Supondremos por ahora que $w_t$ es una secuencia de ruido blanco gaussiana.   
_b)_ Si el modelo es correcto, ¿cuál es el incremento anual promedio estimado en las ganancias registradas por acción?
_c)_ Si el modelo es correcto, ¿la tasa promedio de ganancias registradas aumenta o disminuye del tercer trimestre al cuarto trimestre? y ¿en qué porcentaje aumenta o disminuye?
_d)_ ¿Qué sucede si incluye un término de intersección en el modelo en _a)_? Explique por qué hubo un problema.
_e)_ Grafique los datos, $x_t$, y superponga los valores ajustados, digamos $\hat{x}_t$, en el gráfico. Examine los residuos, $x_t − \hat{x}_t$, y establezca sus conclusiones. ¿Parece que el modelo se ajusta bien a los datos (los residuos se ven blancos)?

La figura \ref{fig:ex01} muestra las ganancias trimestrales por acción de la empresa estadounidense _Johnson & Johnson_, proporcionada por el profesor Paul Griffin (comunicación personal) de la _Graduate School of Management_ de la Universidad de California, Davis. Hay 84 trimestres (21 años) medidos desde el primer trimestre de 1960 hasta el último trimestre de 1980.

```{r p02-01-01, label="fig:p02-01"}
autoplot(jj) +
  ggtitle("Pasageros de la Clase Economica: Melbourne-Sydney") +
  xlab("Año") +
  ylab("Miles") +
  theme_light()
```

Para transformar los datos, reorganizo los datos a un fromato largo, y creo los regresores para el elemento de tendencia (usando ```lubridate::year```) y para los cuartos de cada año (usando ```lubridate::quarter```). Luego, para este ultimo, se crearon 4 variables distintas $Q_i$ usando una expresion condicional sobe ```quarter```.

```{r p02-01-02}
library(lubridate)

# Transformando ls datos para obtener y_t
# y generar los regresores t y Q_i
df_jj <- jj |>
	tsibble::as_tsibble(key=c("Q1", "Q2", "Q3", "Q4")) |>
	mutate(log_Earnings = log(value), 
		year = year(index), 
		quarter=factor(quarter(index)), 
		Q1=ifelse(quarter == 1, 1, 0),
		Q2=ifelse(quarter == 2, 1, 0),
		Q3=ifelse(quarter == 3, 1, 0),
		Q4=ifelse(quarter == 4, 1, 0))
```

Ajustando el modelo de regresión, arroja los siguientes resultados mostrados en la tabla \ref{tab:p02-01}. Los resultados del ajuste se interpretan tomando en cuenta el uso de variables indicadoras: ```year``` corresponde al elemento de tendencia $\beta$, y cada ```Q1, Q2, Q3``` y ```Q4``` corresponde a los valores de $\alpha_i$.

```{r p02-01-03}
library(broom)

# Ajuste del modelo
# mod <- lm(log_Earnings ~ year + quarter, df_jj)
mod <- lm(log_Earnings ~ year + Q1 + Q2 + Q3 + Q4 - 1, df_jj)

# Resultados de la estimacion.
tidy(mod) |> 
	kable(digits=4, 
		col.names=c("Regresor", bquote(beta["i"]), bquote(sigma["i"]), "Z", "p"),
		caption="Resultados de la regresión lineal: Estimadores para los coeficientes del modelo.", 
		label="tab:p02-01")
```

Los resultados muestran que todos los coeficientes son significativos, con $p$-valores mucho menores a $0{,}01$. Es de hacer notar que la diferencia entre la contribución de cada cuarto a las ganancias (en escala logarítmica) es bastante similar, variando solo en las décimas o centésimas.  
Los cambios anuales en las ganancias viene dado por el coeficiente para $t$ (```year```) de la siguiente forma:

$$x_t - x_{t-1} = \beta t - \beta (t-1) = \beta$$

donde los terminos para los cuartos se cancelan en la diferencia. Usando $log(y_t) - log(y_t) = log(y_t/y_{t-1]})$ permite obtener:

$$y_t = y_{t-1}e^\beta$$

Es decir, anualmente hay un incremento promedio de $e^\beta = `r round(exp(coef(mod)[["year"]]), 4)`$ en las ganancias por accion, suponiendo que el modelo es correcto.   
Los incrementos/decrementos de cuarto a cuarto en un mismo año vienen dados por:

$$x_{t,Q_i} - x_{t-1, Q_{i-1}} = \alpha_i - \alpha_{i-1}$$

y al devolver la transformacion, como antes, se obtiene:

$$y_{t, Q_i} = y_{t, Q_{i-1}}e^{\alpha_i - \alpha_{i-1}}$$

Para el incremento del cuarto cuarto al primero, se da un incremento de año de forma que el termino de tendencia no desaparece. Pero por propiedad de exponenciales, este se se puede separar del cambio de cuarto de año como: $y_{t, Q_1} = y_{t-1, Q_4}e^{\beta}e^{\alpha_4 - \alpha_1}$. Los cambios de un cuarto a otro se muestran en la tabla \ref{tab:p02-02}.

```{r p02-01-04}
# Etiquetas para cada uno de los cambios de cuarto
labels <- c(
	bquote(alpha["2"]~"-"~alpha["1"]), 
	bquote(alpha["3"]~"-"~alpha["2"]), 
	bquote(alpha["4"]~"-"~alpha["3"]), 
	bquote(alpha["1"]~"-"~alpha["4"]))
# Incrementos de cuarto a cuarto
increment <- exp(diff(coef(mod)[c("Q1", "Q2", "Q3", "Q4", "Q1")]))

# Tabla de incrementos cuarto a cuarto
tibble(Etiqueta=labels, Incremento=increment) %>%
	mutate(Porcentaje=100 * (Incremento - 1)) %>%
	kable(digits=4,
		caption="Incrementos en las ganancias promedio por accion cuarto a cuarto.", 
		label="tab:p02-02")
```

Como se observa, a excepción del paso del tercer cuarto al cuarto, siempre ocurre un incremento en las ganancias promedio por acción: de $7{,}24$% en el $Q2$, de $11{,}8$% en el $Q3$, y un incremento de $4{,}62$% al pasar al $Q1$. Los resultados muestran que la caída en las ganancias al pasar al $Q4$ es el mayor cambio en las ganancias por acción en cada año, de $20{,}3$%. 

Al intentar añadir un coeficiente, el ajuste es capaz de determinarlo, pero arroja ```NA``` para el coeficiente $\alpha_4$. 
La razon de esto es que al añadir el termino para el coeficiente, este se toma como un caso base (en este caso, $Q1$. 
El coeficiente estimado corresponde entonces a $\alpha_1$), y cada uno de los términos $Q_i$ para $i=2, 3, 4$ se determinan como cambios (cuanto por encima o por debajo del intercepto) con respecto al intercepto: 
de forma que ```Q1``` en el modelo con intercepto es en realidad la diferencia $\alpha_2 - \alpha_1$; ```Q2``` en el modelo con intercepto es en realidad la diferencia $\alpha_3 - \alpha_1$; y ```Q3``` en el modelo con intercepto es en realidad la diferencia $\alpha_4 - \alpha_1$. Pero dado que ya se sacaron todas las diferencias entre cada cuarto con respecto al caso base (primer cuarto), el ultimo coeficiente no significa nada, arrojando el valor de ```NA```.

El gráfico para el modelo se observa en la figura \@ref(fig:p02-02), donde se observa que la serie ajustada esta ligeramente mas suavizada que la serie observada, aunque se nota que de 1962 a 1965 la series ajustada parece sobrestimar de manera sistemática la serie observada, mientras que de 1970 a 1975 el modelo parece subestimar de forma sistemática la serie observada. El resto del tiempo, la serie parece variar por encima o por debajo.

```{r p02-01-05, label="fig:p02-02"}
augmented_df <- left_join(df_jj, augment(mod))

ggplot(df_jj[, c("index", "log_Earnings")], aes(x=as.Date(index), y=log_Earnings)) +
	geom_line() + geom_point() + 
	geom_line(aes(y=.fitted), 
		data=augmented_df[, c("index", ".fitted")],
		color="orange") + 
	geom_point(aes(y=.fitted), 
		data=augmented_df[, c("index", ".fitted")], 
		color="orange") +
  ggtitle("Pasageros de la Clase Economica: Melbourne-Sydney (escala logaritmica)") +
  xlab("Año") +
  ylab("log Ganancias por Accion") + 
  theme_light()
```

Al verificar los residuales, se observa en la figura \@ref(fig:p02-03) que la serie tiene un comportamiento que no se toma en cuenta en el modelo, alguna correlacion entre los valores adyacentes. Tambien se observan valores con mas de dos desviaciones estandar, y en general cambios mas violentos, al inicio y en el centro de la serie.

```{r p02-01-06, label="fig:p02-03"}
ggplot(augmented_df[, c(".fitted", ".std.resid")], 
	aes(x=.fitted, y=.std.resid)) +
	geom_line() + geom_point() +
	geom_hline(yintercept=0) +
	xlab("Valores Estimados") +
  ylab("Residuales Estandarizados") + 
  theme_light()
```

Las gráficas de la figura \@ref(fig:p02-04) muestran las función de autocorrelación y autocorrelación parcial. Se observa de inmediato que la caída de la primera es bastante lenta, indicando una correlación bastante grande entre elementos adyacentes en la serie; mientras que la función de autocorrelación parcial muestra que la correlación parece ser mayor solo entre observaciones separadas a intervalo de un año.

```{r p02-01-07, label="fig:p02-04"}
# Se calculan las correlaciones y correlaciones parciales
df_values <- data.frame(lag=1:19, 
	acf_vals=acf(augmented_df$log_Earnings, plot=FALSE)$acf[,,1][1:19],
	pacf_vals=pacf(augmented_df$log_Earnings, plot=FALSE)$acf[,,1])

cowplot::plot_grid(
	ggplot(data = df_values, mapping = aes(x = lag, y = acf_vals)) +
	  geom_point() +
	  geom_hline(aes(yintercept = 0)) +
	  geom_segment(mapping = aes(xend = lag, yend = 0)) +
	  geom_hline(yintercept = c(1/sqrt(20), -1/sqrt(20)), linetype=2, color='blue') +
	  ylab(latex2exp::TeX("$\\rho(s, t)$")) +
	  theme_light(), 
  ggplot(data = df_values, mapping = aes(x = lag, y = pacf_vals)) +
	  geom_point() +
	  geom_hline(aes(yintercept = 0)) +
	  geom_hline(yintercept = c(1/sqrt(20), -1/sqrt(20)), linetype=2, color='blue') +
	  geom_segment(mapping = aes(xend = lag, yend = 0)) +
	  ylab(latex2exp::TeX("$\\rho(s, t)$")) +
	  theme_light(), 
	nrow=1)
```

La información mostrada en las funciones de autocorrelación y en la distribución de los residuales parece indicar que el modelo no es del todo correcto y que sera apropiado ajustar un modelo autoregresivo.

**Problema 2.2** Para los datos de mortalidad cardiovascular:  
_a)_ Agregue otro componente a la regresión que represente el conteo de partículas cuatro semanas antes; es decir, agregue $P_{t−4}$. Exprese su conclusión.
_b)_ Dibuje una matriz de diagrama de dispersión de $M_t$, $T_t$, $P_t$ y $P_{t−4}$ y luego calcule las correlaciones por pares entre las series. Compare la relación entre $M_t$ y $P_t$ versus $M_t$ y $P_{t−4}$.

```{r p02-02-01, echo=FALSE}
tempR <- tempfile(fileext = ".R")
library(knitr)
purl("Pollution-Mortality-example.Rmd", output=tempR)
source(tempR)
unlink(tempR)
```

La primera parte del análisis se encuentra en el archivo de ejemplo para [la mortalidad cardiovascular y partículas contaminantes]. Allí, se muestra que el mejor modelo encontrado para mortalidad fue:

$$Mt &= `r round(coef_matrix[1,1], 1)` + `r round(coef_matrix[2,1], 3)`t + `r round(coef_matrix[3,1], 3)`(T_t − T_\dot) + `r round(coef_matrix[4,1], 3)`(T_t − T_\dot)^2 + `r round(coef_matrix[5,1], 3)`P_t + w_t$$

En esta parte se busca añadir la información del retraso $P_{t-4}$ para verificar si hay una mejora en el ajuste. Los resultados se muestran a continuación:

```{r p02-02-02}
df_ts <- cbind(part, tempr, cmort)

# Se crea la variable retraso.
pt_4 <- ts.intersect(df_ts, pt_4=stats::lag(df_ts[, "part"],-4), dframe=TRUE)
colnames(pt_4) <- c("Particulas", "Temperatura", "Mortalidad", "P_t-4")

# Datos para la regresión
df_lag <- pt_4 %>%
	mutate(trend = time(cmort)[-(505:508)], 
		diff_Temp=Temperatura - mean(Temperatura),
		diff_Temp_Square=diff_Temp ** 2)

mod_lag <- lm(Mortalidad ~ trend + diff_Temp + diff_Temp_Square + Particulas + `P_t-4`, df_lag)
bind_rows(fitted_models[4,], glance(mod_lag)) %>%
	mutate(SSE=sigma ** 2 * df.residual, MSE=sigma ** 2) %>%
	dplyr::select(SSE, df.residual, MSE, adj.r.squared, AIC, BIC) %>%
	mutate(AIC=AIC / nrow(df_ts_tidy) - log(2*pi), BIC=BIC / nrow(df_ts_tidy) - log(2*pi)) %>%
	tibble::add_column(Model=c(
		"$M_t = \\beta_0 + \\beta_1 t + \\beta_2(T - T_\\dot) + \\beta_3(T - T_\\dot)^2 + \\beta_4 P_t + w_t$", "$M_t = \\beta_0 + \\beta_1 t + \\beta_2(T - T_\\dot) + \\beta_3(T - T_\\dot)^2 + \\beta_4 P_t + P_{t-4} + w_t$"), .before=1) %>%
	kable(digits=3,
		col.names=c("", "SSE", "df", "MSE", "R^2", "AIC", "BIC"),
		caption="Medidas de ajuste y de información para los modelos ajustados.", 
		label="tab:p02-02-01", escape=FALSE)
```

Los resultados muestran que hay una mejora en el ajuste al añadir a $P_{t-4}$, pero que solo resulta en un aumento de la varianza explicada de `r 100 * (0.604 - 0.592)`%. Podemos verificar si el modelo es significativo por medio de la prueba $F$:

$$F(5, 498) = \frac{(40.8 - 39.5) / 5}{39.5 / 498} = 1.3218\times 10^{-5}$$

Este valor de $F$ tiene una probabilidad asociada de $F(5, 498) = `r round(1 - pf((40.8 - 39.5) / 5 / 39.5 / 498, 5, 498), 4)`$. Esto quiere decir que no hay una mejora significativa en el modelo al añadir a $P_{t-4}$ comparado con el modelo sin este termino.

Al realizar los gráficos de dispersión con los pares de variables del modelo, y calcular el coeficiente de correlación de estas, se puede observar que la correlación entre $M_t$ y $P_t$ versus $M_t$ y $P_{t−4}$, es bastante similar, siendo mayor par el ultimo caso. Dada esta correlación, seria mas apropiado elegir un modelo en el cual solo se haga una relación entre $M_t$ y $P_{t−4}$, eliminando el termino de $P_t$.

```{r p02-02-03, label="fig:p02-02-03"}
if (!require("GGally"))
	install.packages("GGally")

pt_4 |> GGally::ggpairs()
```

**Problema 2.3** En este problema, exploramos la diferencia entre una caminata aleatoria y un proceso estacionario de tendencia.  
_a)_ Genere cuatro series que sean paseo aleatorio con deriva, de longitud $n = 100$ con $\delta = {,}01$ y $\sigma_w = 1$. Llame a los datos $x_t$ para $t = 1, \ldots, 100$. Ajuste la regresión $x_t = \beta t + w_t$ usando mínimos cuadrados. Grafique los datos, la función media verdadera (es decir, $\mu t = {,}01 t$) y la línea ajustada, $\hat{x}_t = \hat{\beta} t$, en el mismo gráfico.   
_b)_ Genere cuatro series de longitud $n = 100$ que sean de tendencia lineal más ruido, digamos $y_t = {,}01 t + w_t$, donde $t$ y $w_t$ son como en la parte _a)_. Ajuste la regresión $y_t = \beta t + w_t$ usando mínimos cuadrados. Grafique los datos, la función media verdadera y la línea ajustada.   
_c)_ Comente (qué aprendió de esta tarea).   

En la siguiente figura se observan los graficos para los incisos _a)_ y _b)_:

```{r p02-03-01}
set.seed(24644350)
sim_series <- list(xt_1 = ts(cumsum(rnorm(100, .01))),
	xt_2 = ts(cumsum(rnorm(100, .01))),
	xt_3 = ts(cumsum(rnorm(100, .01))),
	xt_4 = ts(cumsum(rnorm(100, .01)))
  )

# Regresiones
fitted_values <- purrr::map(sim_series, ~fitted(lm(.~ time(.))))

# Joining
p1 <- bind_rows(sim_series) %>% 
  tidyr::gather(key="serie", value="sim") %>%
  bind_cols( 
  	bind_cols(fitted_values) %>% 
  		tidyr::gather(key="serie", value="fitted") %>%
  		select(-serie)) %>%
  ggplot(aes(x=rep(1:100L, 4), y=sim, colour=serie)) +
    geom_line() + geom_line(aes(y=fitted)) + 
    geom_abline(slope=.1, linetype=2) + 
    xlab("") + ylab("") +
    theme_light() +
    theme(legend.position="none")

sim_series <- list(xt_1 = ts(.1 * 1:100 + rnorm(100)),
	xt_2 = ts(.1 * 1:100 + rnorm(100)),
	xt_3 = ts(.1 * 1:100 + rnorm(100)),
	xt_4 = ts(.1 * 1:100 + rnorm(100))
  )

# Regresiones
fitted_values <- purrr::map(sim_series, ~fitted(lm(.~ time(.))))

# Joining
p2 <- bind_rows(sim_series) %>% 
  tidyr::gather(key="serie", value="sim") %>%
  bind_cols( 
  	bind_cols(fitted_values) %>% 
  		tidyr::gather(key="serie", value="fitted") %>%
  		select(-serie)) %>%
  ggplot(aes(x=rep(1:100L, 4), y=sim, colour=serie)) +
    geom_line() + geom_line(aes(y=fitted)) + 
    geom_abline(slope=.1, linetype=2) + 
    xlab("") + ylab("") +
    theme_light() +
    theme(legend.position="none")

cowplot::plot_grid(p1, p2, labels=c("a)", "b)"))
```

Se puede ver que los procesos que son paseos aleatorios son completamnete distintos a un proceso con tendencia lienal. En el caso de los paseos aleatorios, la serie se genera de un proceso normal con valor medio dado por el _drift_, donde cada nuevo valor es dependiente unicamente del valor anterior en el proceso. 
Por otro lado, el modelo con tendencia sigue una estructura de dependencia que solo depende del timepo, pero no de valores anteriores de la serie. Lo cual hace que la serie solo fluctue alrededor de su valor medio.

**Problema 2.4 Información de Kullback-Leibler**. Dado el vector aleatorio $n\times1$ $y$, definimos la información para discriminar entre dos densidades en la misma familia, indexadas por un parámetro $\theta$, digamos $f(y; \theta_1)$ y $f(y; \theta_2)$, como:

$$I(\theta_1;\tehta_2) = n^{-1}E_1 log\frac{f(y; \theta_1)}{f(y; \theta_2)}$$

donde $E_1$ denota expectativa con respecto a la densidad determinada por $\theta_1$. Para el modelo de regresión gaussiana, los parámetros son $\theta = (\beta\′, \sigma^2)\′$. Muestra que:

$$I(\theta_1;\tehta_2) = \frac{1}{2}\left(\frac{\sigma^2_1}{\sigma^2_2} - log\frac{\sigma^2_1}{\sigma^2_2} - 1\right) + \frac{1}{2}\frac{(\beta_1 - \beta_2)\′Z\′Z(\beta_1 - \beta_2)}{n\sigma_2^2}$$


**Problema 2.5 Selección del modelo.** Ambos criterios de selección (2.15) y (2.16) se derivan de argumentos teóricos de la información, basados en los bien conocidos números de discriminación de información de Kullback-Leibler. Consideramos que la medida dada por la ecuación obtenida en el problema anterior mide la discrepancia entre las dos densidades, caracterizada por los valores de los parámetros $\theta_1\′ = (\beta_1\′, \sigma_1^2)\′$ y $\theta_2\′ = (\beta_2\′, \sigma_2^2)\′$. Ahora, si el verdadero valor del vector de parámetros es $\theta_1$, argumentamos que el mejor modelo sería uno que minimice la discrepancia entre el valor teórico y la muestra, digamos $I(\theta_1; \hat{\theta})$. Debido a que no se conocerá $\theta_1$, Hurvich y Tsai consideraron encontrar un estimador insesgado para $E_1[I(\beta_1, \sigma_1^2; \hat{β},\hat{σ}^2)]$, donde

$$I(\beta_1, \sigma_1^2; \hat{β},\hat{σ}^2) = \frac{1}{2}\left(\frac{\sigma^2_1}{\sigma^2_2} - log\frac{\sigma^2_1}{\sigma^2_2} - 1\right) + \frac{1}{2}\frac{(\beta_1 - \beta_2)\′Z\′Z(\beta_1 - \beta_2)}{n\sigma_2^2}$$

**Problema 2.11** Utilice dos técnicas de suavizado diferentes para estimar la tendencia en la serie de temperatura global ```globtemp```. Comente.



# Capitulo 3.

**Problema 3.1** Para un $MA(1)$, $x_t = w_t + \theta w_{t−1}$, demuestre que $\vert\rho_x(1)\vert \le 1/2$ para cualquier número $\theta$. ¿Para qué valores de $\theta$ $\rho_x(1)$ alcanza su máximo y mínimo?

https://www.youtube.com/watch?v=emF1iBcni0U&ab_channel=AnaMetriks
https://liclourdescuellar.ruplayers.com/
https://finance-r.netlify.app/quantmod.html